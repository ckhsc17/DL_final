{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TA0BeBGXcfQL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f'使用 GPU：{torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('使用 CPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFiWYCJcrCE",
        "outputId": "e2c45937-cbcc-4c0f-892f-2efef9e19ff7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用 GPU：NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31X6_C5edbbw",
        "outputId": "32cecf46-c538-4c7a-832e-9fff605f2304"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Database\")"
      ],
      "metadata": {
        "id": "WHIpSXXwdiSS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG4_1RIwed0T",
        "outputId": "ab945173-338f-4ece-97e8-304ce64c29cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: peft\n",
            "Version: 0.15.2\n",
            "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
            "Home-page: https://github.com/huggingface/peft\n",
            "Author: The HuggingFace team\n",
            "Author-email: benjamin@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oSTCid07ey-i",
        "outputId": "12a17773-11da-4fd9-fae7-4e4d609eeffc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.7.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.32.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "590675eb5f3a4a638788c87ec01e31f4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Dict, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType  # 只留 LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 固定亂數種子，確保結果可重現\n",
        "# =============================================================================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 資料載入與 Label Processing\n",
        "# =============================================================================\n",
        "def load_and_prepare(subjects: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    讀取多個 subject 的資料，合併後做 label encoding：\n",
        "      - label_subject\n",
        "      - label_chapter\n",
        "      - label_section\n",
        "    \"\"\"\n",
        "    def load_and_merge(subject: str) -> pd.DataFrame:\n",
        "        base_path = f\"{subject}_Database\"\n",
        "        qdf = pd.read_csv(f\"{base_path}/{subject}_question_bank.csv\")\n",
        "        cdf = pd.read_csv(f\"{base_path}/{subject}_chapter_list.csv\")\n",
        "        qdf.columns = qdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        cdf.columns = cdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        df = qdf.merge(cdf, on=\"section_name\", how=\"left\")\n",
        "        df[\"subject\"] = subject\n",
        "        return df\n",
        "\n",
        "    # 合併指定 subjects 的資料\n",
        "    df = pd.concat([load_and_merge(s) for s in subjects], ignore_index=True)\n",
        "    df = df[[\"subject\", \"chapter_name_x\", \"section_name\", \"ques_detl\"]].dropna().reset_index(drop=True)\n",
        "\n",
        "    # subject label\n",
        "    df[\"label_str\"] = df[\"subject\"]\n",
        "    label2id_subject = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_subject = {i: lab for lab, i in label2id_subject.items()}\n",
        "    df[\"label_subject\"] = df[\"label_str\"].map(label2id_subject)\n",
        "\n",
        "    # chapter label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"]\n",
        "    label2id_chapter = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_chapter = {i: lab for lab, i in label2id_chapter.items()}\n",
        "    df[\"label_chapter\"] = df[\"label_str\"].map(label2id_chapter)\n",
        "\n",
        "    # section label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"] + \"::\" + df[\"section_name\"]\n",
        "    label2id_section = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_section = {i: lab for lab, i in label2id_section.items()}\n",
        "    df[\"label_section\"] = df[\"label_str\"].map(label2id_section)\n",
        "\n",
        "    # 只保留至少出現兩次的 section\n",
        "    vc = df[\"label_section\"].value_counts()\n",
        "    valid_secs = set(vc[vc >= 2].index)\n",
        "    df = df[df[\"label_section\"].isin(valid_secs)].reset_index(drop=True)\n",
        "\n",
        "    return df, (label2id_subject, id2label_subject), (label2id_chapter, id2label_chapter), (label2id_section, id2label_section)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 自訂 Dataset\n",
        "# =============================================================================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        labels: List[int],\n",
        "        tokenizer,\n",
        "        max_len: int = 128,\n",
        "        mode: str = \"flat_chapter\",  # 'flat_chapter', 'flat_section', 'hierarchical'\n",
        "        subject_labels: List[int] = None\n",
        "    ):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.mode = mode\n",
        "        self.subject_labels = subject_labels  # 只有 hierarchical 模式才需要\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        if self.mode == \"hierarchical\":\n",
        "            item[\"subject_labels\"] = torch.tensor(self.subject_labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# =============================================================================\n",
        "# 4. TextCNN Model 定義\n",
        "# =============================================================================\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_classes: int,\n",
        "        kernel_sizes: List[int] = [3,4,5],\n",
        "        num_filters: int = 100,\n",
        "        dropout_p: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (B, L)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.embedding(input_ids)        # (B, L, D)\n",
        "        x = x.permute(0, 2, 1)               # (B, D, L)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            c = conv(x)                      # (B, F, L - k + 1)\n",
        "            c = torch.relu(c)                # (B, F, L - k + 1)\n",
        "            c = torch.max_pool1d(c, kernel_size=c.size(2))  # (B, F, 1)\n",
        "            conv_outs.append(c.squeeze(2))   # (B, F)\n",
        "        cat = torch.cat(conv_outs, dim=1)    # (B, F * len(kernel_sizes))\n",
        "        drop = self.dropout(cat)             # (B, F * len(kernel_sizes))\n",
        "        logits = self.fc(drop)               # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 基礎 MLP Model 定義 (只用於 flat 模式)\n",
        "# =============================================================================\n",
        "class BasicMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        features: (B, input_dim)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.fc1(features)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 訓練與評估函式\n",
        "# =============================================================================\n",
        "def compute_metrics(preds_and_labels) -> Dict[str, float]:\n",
        "    logits, labels = preds_and_labels\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_macro\": prec,\n",
        "        \"recall_macro\": rec,\n",
        "        \"f1_macro\": f1\n",
        "    }\n",
        "\n",
        "def train_flat_transformer(\n",
        "    model_name: str,\n",
        "    num_labels: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    output_dir: str,\n",
        "    device: torch.device,\n",
        "    use_dora: bool = False\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 flat (Subject+Chapter 或 flat Section) 模式的小型 Transformer (BERT/ RoBERTa)，回傳 metrics。\n",
        "    如果 use_dora=True，會在模型上套 DoRA Adapter（使用 LoraConfig with use_dora=True）。\n",
        "    \"\"\"\n",
        "    # 1. 選擇 tokenizer & base_model\n",
        "    if \"roberta\" in model_name:\n",
        "        tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    else:\n",
        "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    # 2. 如果要用 DoRA，包成 PEFT 模型\n",
        "    if use_dora:\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            inference_mode=False,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.05,\n",
        "            target_modules=[\"query\", \"value\"],  # 對 BERT/RoBERTa 模型，\"query\" 和 \"value\" 即可\n",
        "            use_dora=True,                      # 啟用 DoRA\n",
        "            #distillation_loss_weight=0.2,\n",
        "            #retention_loss_weight=0.1\n",
        "        )\n",
        "        model = get_peft_model(base_model, peft_config)\n",
        "    else:\n",
        "        model = base_model\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # 3. 構造 Dataset\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "\n",
        "    # 4. TrainingArguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=3e-5 if not use_dora else 3e-4,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        logging_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    # 5. 初始化 Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # 6. 訓練 & 驗證\n",
        "    start = time.time()\n",
        "    trainer.train()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    # 7. 儲存 adapter（若有 DoRA）或整個模型\n",
        "    #    如果 use_dora=True，Trainer.save_model() 會把 PEFT adapter 一併儲存到 output_dir\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # 8. 計算檔案大小\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    metrics[\"eval_accuracy\"],\n",
        "        \"precision\":   metrics[\"eval_precision_macro\"],\n",
        "        \"recall\":      metrics[\"eval_recall_macro\"],\n",
        "        \"f1\":          metrics[\"eval_f1_macro\"],\n",
        "        \"train_time\":  elapsed,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "def train_textcnn(\n",
        "    vocab_size: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    tokenizer,\n",
        "    num_labels: int,\n",
        "    output_dir: str,\n",
        "    device: torch.device\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 TextCNN (flat_chapter) 模式，回傳 metrics。\n",
        "    \"\"\"\n",
        "    max_len = 128\n",
        "    batch_size = 32\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    embed_dim = 300\n",
        "    model = TextCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    history = {\"train_loss\": [], \"valid_loss\": [], \"train_f1\": [], \"valid_f1\": []}\n",
        "\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "        _, _, train_f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        total_vloss = 0.0\n",
        "        v_preds, v_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                logits = model(input_ids)\n",
        "                loss = criterion(logits, labels)\n",
        "                total_vloss += loss.item() * input_ids.size(0)\n",
        "                preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "                v_preds.extend(preds)\n",
        "                v_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        valid_loss = total_vloss / len(valid_loader.dataset)\n",
        "        _, _, valid_f1, _ = precision_recall_fscore_support(\n",
        "            v_labels, v_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"valid_loss\"].append(valid_loss)\n",
        "        history[\"train_f1\"].append(train_f1)\n",
        "        history[\"valid_f1\"].append(valid_f1)\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if valid_f1 > best_f1:\n",
        "            best_f1 = valid_f1\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/best_textcnn.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss {train_loss:.4f} | Valid Loss {valid_loss:.4f} | \"\n",
        "              f\"Train F1 {train_f1:.4f} | Valid F1 {valid_f1:.4f}\")\n",
        "\n",
        "    # 存檔： tokenizer vocab\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    None,\n",
        "        \"precision\":   None,\n",
        "        \"recall\":      None,\n",
        "        \"f1\":          best_f1,\n",
        "        \"train_time\":  None,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Pipeline 主程序：整合上述所有 case\n",
        "# =============================================================================\n",
        "def main_pipeline():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. 載入資料與 Labels\n",
        "    df, subj_map, chap_map, sect_map = load_and_prepare([\"math\", \"science\"]) # social\n",
        "    label2id_subject, id2label_subject = subj_map\n",
        "    label2id_chapter, id2label_chapter = chap_map\n",
        "    label2id_section, id2label_section = sect_map\n",
        "\n",
        "    # 2. 切 train/valid/test（90/10 → 再分81/9/10)\n",
        "    base = df[[\"ques_detl\", \"label_subject\", \"label_chapter\", \"label_section\", \"subject\"]].copy()\n",
        "\n",
        "    # 先切出 10% 做最終 test\n",
        "    rest, test = train_test_split(\n",
        "        base, test_size=0.1, stratify=base[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 再把 rest 切成 90/10 ≈ 81%/9% 做 train/valid\n",
        "    train_df, valid_df = train_test_split(\n",
        "        rest, test_size=0.1, stratify=rest[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 取出 text 和各種 label lists\n",
        "    train_texts = train_df[\"ques_detl\"].tolist()\n",
        "    valid_texts = valid_df[\"ques_detl\"].tolist()\n",
        "    test_texts  = test[\"ques_detl\"].tolist()\n",
        "\n",
        "    train_subj_labels = train_df[\"label_subject\"].tolist()\n",
        "    valid_subj_labels = valid_df[\"label_subject\"].tolist()\n",
        "    test_subj_labels  = test[\"label_subject\"].tolist()\n",
        "\n",
        "    train_chap_labels = train_df[\"label_chapter\"].tolist()\n",
        "    valid_chap_labels = valid_df[\"label_chapter\"].tolist()\n",
        "    test_chap_labels  = test[\"label_chapter\"].tolist()\n",
        "\n",
        "    train_sect_labels = train_df[\"label_section\"].tolist()\n",
        "    valid_sect_labels = valid_df[\"label_section\"].tolist()\n",
        "    test_sect_labels  = test[\"label_section\"].tolist()\n",
        "\n",
        "    # 3. 建立 tokenizer (給所有 TextCNN/Transformer 共用)\n",
        "    bert_tok    = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "    roberta_tok = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    # 4. 建立詞彙大小 (TextCNN 用)\n",
        "    vocab_size = bert_tok.vocab_size\n",
        "\n",
        "    # 5. 實驗配置\n",
        "    model_types = [\n",
        "        (\"DoRA_BERT\",    \"bert-base-uncased\", True),\n",
        "        (\"DoRA_RoBERTa\", \"roberta-base\",       True),\n",
        "        (\"BERT\",         \"bert-base-uncased\", False),\n",
        "        (\"RoBERTa\",      \"roberta-base\",       False),\n",
        "    ]\n",
        "    others = [\"TextCNN\", \"MLP\"]\n",
        "\n",
        "    # 6. 三種分類策略\n",
        "    strategies = [\n",
        "        \"flat_chapter\",          # 直接分類 subject+chapter\n",
        "        \"flat_section_then_map\", # 先分類 subject+chapter+section，再 map 回 chapter\n",
        "        \"hierarchical\"           # 先分 subject，再分 chapter\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_name, hf_model_name, use_dora in model_types:\n",
        "        for strat in strategies:\n",
        "            exp_name = f\"{model_name}__{strat}\"\n",
        "            output_dir = f\"./outputs/{exp_name}\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            print(f\"\\n▶開始實驗: {exp_name}\")\n",
        "\n",
        "            if strat == \"flat_chapter\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_chapter),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_chap_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_chap_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"flat_section_then_map\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_section),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_sect_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_sect_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                # 如需計算 chapter fine，須先批次推論再做 mapping\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"hierarchical\":\n",
        "                sub_output = os.path.join(output_dir, \"subject_model\")\n",
        "                chap_output = os.path.join(output_dir, \"chapter_model\")\n",
        "                os.makedirs(sub_output, exist_ok=True)\n",
        "                os.makedirs(chap_output, exist_ok=True)\n",
        "\n",
        "                # 1) 訓練 subject\n",
        "                sub_metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_subject),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_subj_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_subj_labels,\n",
        "                    output_dir=sub_output,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "\n",
        "                # 2) 針對每個 subject 訓練 chapter classifier\n",
        "                chap_metrics = {}\n",
        "                for subj_id, subj_name in id2label_subject.items():\n",
        "                    idx_train = [i for i, s in enumerate(train_subj_labels) if s == subj_id]\n",
        "                    idx_valid = [i for i, s in enumerate(valid_subj_labels) if s == subj_id]\n",
        "\n",
        "                    sub_train_texts = [train_texts[i] for i in idx_train]\n",
        "                    sub_train_chaps = [train_chap_labels[i] for i in idx_train]\n",
        "                    sub_valid_texts = [valid_texts[i] for i in idx_valid]\n",
        "                    sub_valid_chaps = [valid_chap_labels[i] for i in idx_valid]\n",
        "\n",
        "                    unique_chaps = sorted({train_chap_labels[i] for i in idx_train})\n",
        "                    if len(unique_chaps) < 2:\n",
        "                        continue\n",
        "\n",
        "                    sub_dir = os.path.join(chap_output, f\"subj_{subj_id}\")\n",
        "                    os.makedirs(sub_dir, exist_ok=True)\n",
        "                    sub_chap_m = train_flat_transformer(\n",
        "                        model_name=hf_model_name,\n",
        "                        num_labels=len(unique_chaps),\n",
        "                        train_texts=sub_train_texts,\n",
        "                        train_labels=sub_train_chaps,\n",
        "                        valid_texts=sub_valid_texts,\n",
        "                        valid_labels=sub_valid_chaps,\n",
        "                        output_dir=sub_dir,\n",
        "                        device=device,\n",
        "                        use_dora=use_dora\n",
        "                    )\n",
        "                    chap_metrics[subj_id] = sub_chap_m\n",
        "\n",
        "                results.append((exp_name, {\"subject\": sub_metrics, \"chapter\": chap_metrics}))\n",
        "\n",
        "    # TextCNN + MLP 只負責 flat_chapter\n",
        "    for model_name in others:\n",
        "        strat = \"flat_chapter\"\n",
        "        exp_name = f\"{model_name}__{strat}\"\n",
        "        output_dir = f\"./outputs/{exp_name}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"\\n▶開始實驗: {exp_name}\")\n",
        "\n",
        "        if model_name == \"TextCNN\":\n",
        "            start = time.time()\n",
        "            metrics = train_textcnn(\n",
        "                vocab_size=vocab_size,\n",
        "                train_texts=train_texts,\n",
        "                train_labels=train_chap_labels,\n",
        "                valid_texts=valid_texts,\n",
        "                valid_labels=valid_chap_labels,\n",
        "                tokenizer=bert_tok,\n",
        "                num_labels=len(label2id_chapter),\n",
        "                output_dir=output_dir,\n",
        "                device=device\n",
        "            )\n",
        "            elapsed = time.time() - start\n",
        "            metrics[\"train_time\"] = elapsed\n",
        "            results.append((exp_name, metrics))\n",
        "\n",
        "        elif model_name == \"MLP\":\n",
        "            train_enc = bert_tok(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            valid_enc = bert_tok(valid_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "            bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                \"bert-base-uncased\", output_hidden_states=True\n",
        "            )\n",
        "            bert_model.to(device)\n",
        "            bert_model.eval()\n",
        "\n",
        "            def encode_avg(inputs):\n",
        "                input_ids = inputs[\"input_ids\"].to(device)\n",
        "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "                with torch.no_grad():\n",
        "                    outputs = bert_model.bert(input_ids, attention_mask=attention_mask)\n",
        "                    last_hidden = outputs.last_hidden_state  # (B, L, D)\n",
        "                    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "                    summed = torch.sum(last_hidden * mask, 1)\n",
        "                    counts = torch.clamp(mask.sum(1), min=1e-9)\n",
        "                    avg_pooled = summed / counts\n",
        "                    return avg_pooled.cpu()\n",
        "\n",
        "            train_feats = encode_avg(train_enc)\n",
        "            valid_feats = encode_avg(valid_enc)\n",
        "\n",
        "            input_dim = train_feats.size(1)\n",
        "            hidden_dim = 256\n",
        "            model = BasicMLP(input_dim, hidden_dim, num_classes=len(label2id_chapter))\n",
        "            model.to(device)\n",
        "\n",
        "            train_labels_tensor = torch.tensor(train_chap_labels)\n",
        "            valid_labels_tensor = torch.tensor(valid_chap_labels)\n",
        "\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            start = time.time()\n",
        "            best_f1 = 0.0\n",
        "            for epoch in range(1, 6):\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(train_feats.to(device))\n",
        "                loss = criterion(logits, train_labels_tensor.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    v_logits = model(valid_feats.to(device))\n",
        "                    v_preds = torch.argmax(v_logits, dim=1).cpu().numpy()\n",
        "                    v_labels = valid_labels_tensor.numpy()\n",
        "                    _, _, v_f1, _ = precision_recall_fscore_support(v_labels, v_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "                if v_f1 > best_f1:\n",
        "                    best_f1 = v_f1\n",
        "                    torch.save(model.state_dict(), f\"{output_dir}/best_mlp.pt\")\n",
        "                print(f\"Epoch {epoch} | Valid F1 {v_f1:.4f}\")\n",
        "\n",
        "            elapsed = time.time() - start\n",
        "            total_size = 0\n",
        "            for root, _, files in os.walk(output_dir):\n",
        "                for fname in files:\n",
        "                    total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "            results.append((exp_name, {\n",
        "                \"accuracy\":    None,\n",
        "                \"precision\":   None,\n",
        "                \"recall\":      None,\n",
        "                \"f1\":          best_f1,\n",
        "                \"train_time\":  elapsed,\n",
        "                \"model_size\":  total_size\n",
        "            }))\n",
        "\n",
        "    # 8. 把所有實驗結果匯出\n",
        "    out_df = pd.DataFrame([{\"experiment\": name, **metrics} for name, metrics in results])\n",
        "    out_df.to_csv(\"experiment_results.csv\", index=False)\n",
        "    print(\"\\n所有實驗完成，結果已存到 experiment_results.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CjcNuWk1dn0u",
        "outputId": "13630126-ffcc-4d44-f8a8-5ff0051f814a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗: DoRA_BERT__flat_chapter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-8-ea6bff074298>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6736' max='6736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6736/6736 16:32, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.149200</td>\n",
              "      <td>2.792263</td>\n",
              "      <td>0.207081</td>\n",
              "      <td>0.076804</td>\n",
              "      <td>0.105434</td>\n",
              "      <td>0.074639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.538900</td>\n",
              "      <td>2.272581</td>\n",
              "      <td>0.311289</td>\n",
              "      <td>0.174444</td>\n",
              "      <td>0.191384</td>\n",
              "      <td>0.154444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.209300</td>\n",
              "      <td>2.021570</td>\n",
              "      <td>0.410154</td>\n",
              "      <td>0.264218</td>\n",
              "      <td>0.273097</td>\n",
              "      <td>0.240184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.959700</td>\n",
              "      <td>1.791495</td>\n",
              "      <td>0.476954</td>\n",
              "      <td>0.386619</td>\n",
              "      <td>0.339415</td>\n",
              "      <td>0.328136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.787500</td>\n",
              "      <td>1.733930</td>\n",
              "      <td>0.478958</td>\n",
              "      <td>0.349968</td>\n",
              "      <td>0.351053</td>\n",
              "      <td>0.325956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.668000</td>\n",
              "      <td>1.606124</td>\n",
              "      <td>0.523714</td>\n",
              "      <td>0.384601</td>\n",
              "      <td>0.392700</td>\n",
              "      <td>0.369475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.585800</td>\n",
              "      <td>1.561748</td>\n",
              "      <td>0.544422</td>\n",
              "      <td>0.407062</td>\n",
              "      <td>0.411827</td>\n",
              "      <td>0.393817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.526900</td>\n",
              "      <td>1.552831</td>\n",
              "      <td>0.539078</td>\n",
              "      <td>0.417687</td>\n",
              "      <td>0.405769</td>\n",
              "      <td>0.389944</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗: DoRA_BERT__flat_section_then_map\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-8-ea6bff074298>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6736' max='6736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6736/6736 16:35, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.209000</td>\n",
              "      <td>3.709156</td>\n",
              "      <td>0.130929</td>\n",
              "      <td>0.032137</td>\n",
              "      <td>0.055551</td>\n",
              "      <td>0.031257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.535900</td>\n",
              "      <td>3.280009</td>\n",
              "      <td>0.189045</td>\n",
              "      <td>0.051447</td>\n",
              "      <td>0.091451</td>\n",
              "      <td>0.055788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.166800</td>\n",
              "      <td>2.994778</td>\n",
              "      <td>0.236473</td>\n",
              "      <td>0.112187</td>\n",
              "      <td>0.121835</td>\n",
              "      <td>0.094315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.911400</td>\n",
              "      <td>2.767603</td>\n",
              "      <td>0.303273</td>\n",
              "      <td>0.162794</td>\n",
              "      <td>0.173778</td>\n",
              "      <td>0.148120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.709800</td>\n",
              "      <td>2.617620</td>\n",
              "      <td>0.343353</td>\n",
              "      <td>0.194172</td>\n",
              "      <td>0.205708</td>\n",
              "      <td>0.174717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.561400</td>\n",
              "      <td>2.516973</td>\n",
              "      <td>0.363393</td>\n",
              "      <td>0.214870</td>\n",
              "      <td>0.223808</td>\n",
              "      <td>0.195473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.448300</td>\n",
              "      <td>2.469521</td>\n",
              "      <td>0.376754</td>\n",
              "      <td>0.225238</td>\n",
              "      <td>0.240891</td>\n",
              "      <td>0.208715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.384600</td>\n",
              "      <td>2.422845</td>\n",
              "      <td>0.405478</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.259907</td>\n",
              "      <td>0.231681</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗: DoRA_BERT__hierarchical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-8-ea6bff074298>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6736' max='6736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6736/6736 16:36, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.118100</td>\n",
              "      <td>0.062055</td>\n",
              "      <td>0.984636</td>\n",
              "      <td>0.983145</td>\n",
              "      <td>0.984623</td>\n",
              "      <td>0.983871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>0.049061</td>\n",
              "      <td>0.986640</td>\n",
              "      <td>0.984337</td>\n",
              "      <td>0.987817</td>\n",
              "      <td>0.986004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.033000</td>\n",
              "      <td>0.041935</td>\n",
              "      <td>0.989312</td>\n",
              "      <td>0.990348</td>\n",
              "      <td>0.987210</td>\n",
              "      <td>0.988727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.024700</td>\n",
              "      <td>0.040434</td>\n",
              "      <td>0.992652</td>\n",
              "      <td>0.993052</td>\n",
              "      <td>0.991498</td>\n",
              "      <td>0.992262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.019600</td>\n",
              "      <td>0.031873</td>\n",
              "      <td>0.992652</td>\n",
              "      <td>0.993052</td>\n",
              "      <td>0.991498</td>\n",
              "      <td>0.992262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.015300</td>\n",
              "      <td>0.036008</td>\n",
              "      <td>0.992652</td>\n",
              "      <td>0.993378</td>\n",
              "      <td>0.991187</td>\n",
              "      <td>0.992257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.009300</td>\n",
              "      <td>0.036319</td>\n",
              "      <td>0.993320</td>\n",
              "      <td>0.993597</td>\n",
              "      <td>0.992355</td>\n",
              "      <td>0.992968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.007600</td>\n",
              "      <td>0.040116</td>\n",
              "      <td>0.992652</td>\n",
              "      <td>0.993052</td>\n",
              "      <td>0.991498</td>\n",
              "      <td>0.992262</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-8-ea6bff074298>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4104' max='4104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4104/4104 10:07, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.521200</td>\n",
              "      <td>1.973681</td>\n",
              "      <td>0.363239</td>\n",
              "      <td>0.302326</td>\n",
              "      <td>0.250060</td>\n",
              "      <td>0.198539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.895400</td>\n",
              "      <td>1.649032</td>\n",
              "      <td>0.459519</td>\n",
              "      <td>0.396140</td>\n",
              "      <td>0.360661</td>\n",
              "      <td>0.332703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.634700</td>\n",
              "      <td>1.475816</td>\n",
              "      <td>0.506565</td>\n",
              "      <td>0.483296</td>\n",
              "      <td>0.417393</td>\n",
              "      <td>0.409331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.461200</td>\n",
              "      <td>1.357810</td>\n",
              "      <td>0.551422</td>\n",
              "      <td>0.508349</td>\n",
              "      <td>0.478792</td>\n",
              "      <td>0.477227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.307700</td>\n",
              "      <td>1.272494</td>\n",
              "      <td>0.583151</td>\n",
              "      <td>0.560277</td>\n",
              "      <td>0.529086</td>\n",
              "      <td>0.526150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.217000</td>\n",
              "      <td>1.207253</td>\n",
              "      <td>0.612691</td>\n",
              "      <td>0.603191</td>\n",
              "      <td>0.565923</td>\n",
              "      <td>0.561131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.141700</td>\n",
              "      <td>1.153723</td>\n",
              "      <td>0.626915</td>\n",
              "      <td>0.591334</td>\n",
              "      <td>0.578720</td>\n",
              "      <td>0.573086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.098100</td>\n",
              "      <td>1.145888</td>\n",
              "      <td>0.636761</td>\n",
              "      <td>0.599352</td>\n",
              "      <td>0.592535</td>\n",
              "      <td>0.585592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29/29 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-8-ea6bff074298>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ea6bff074298>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ea6bff074298>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m                     \u001b[0msub_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchap_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"subj_{subj_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                     sub_chap_m = train_flat_transformer(\n\u001b[0m\u001b[1;32m    547\u001b[0m                         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                         \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_chaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ea6bff074298>\u001b[0m in \u001b[0;36mtrain_flat_transformer\u001b[0;34m(model_name, num_labels, train_texts, train_labels, valid_texts, valid_labels, output_dir, device, use_dora)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;31m# 6. 訓練 & 驗證\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Dict, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType  # 只留 LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 固定亂數種子，確保結果可重現\n",
        "# =============================================================================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 資料載入與 Label Processing\n",
        "# =============================================================================\n",
        "def load_and_prepare(subjects: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    讀取多個 subject 的資料，合併後做 label encoding：\n",
        "      - label_subject\n",
        "      - label_chapter\n",
        "      - label_section\n",
        "    \"\"\"\n",
        "    def load_and_merge(subject: str) -> pd.DataFrame:\n",
        "        base_path = f\"{subject}_Database\"\n",
        "        qdf = pd.read_csv(f\"{base_path}/{subject}_question_bank.csv\")\n",
        "        cdf = pd.read_csv(f\"{base_path}/{subject}_chapter_list.csv\")\n",
        "        qdf.columns = qdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        cdf.columns = cdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        df = qdf.merge(cdf, on=\"section_name\", how=\"left\")\n",
        "        df[\"subject\"] = subject\n",
        "        return df\n",
        "\n",
        "    # 合併指定 subjects 的資料\n",
        "    df = pd.concat([load_and_merge(s) for s in subjects], ignore_index=True)\n",
        "    df = df[[\"subject\", \"chapter_name_x\", \"section_name\", \"ques_detl\"]].dropna().reset_index(drop=True)\n",
        "\n",
        "    # subject label\n",
        "    df[\"label_str\"] = df[\"subject\"]\n",
        "    label2id_subject = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_subject = {i: lab for lab, i in label2id_subject.items()}\n",
        "    df[\"label_subject\"] = df[\"label_str\"].map(label2id_subject)\n",
        "\n",
        "    # chapter label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"]\n",
        "    label2id_chapter = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_chapter = {i: lab for lab, i in label2id_chapter.items()}\n",
        "    df[\"label_chapter\"] = df[\"label_str\"].map(label2id_chapter)\n",
        "\n",
        "    # section label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"] + \"::\" + df[\"section_name\"]\n",
        "    label2id_section = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_section = {i: lab for lab, i in label2id_section.items()}\n",
        "    df[\"label_section\"] = df[\"label_str\"].map(label2id_section)\n",
        "\n",
        "    # 只保留至少出現兩次的 section\n",
        "    vc = df[\"label_section\"].value_counts()\n",
        "    valid_secs = set(vc[vc >= 2].index)\n",
        "    df = df[df[\"label_section\"].isin(valid_secs)].reset_index(drop=True)\n",
        "\n",
        "    return df, (label2id_subject, id2label_subject), (label2id_chapter, id2label_chapter), (label2id_section, id2label_section)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 自訂 Dataset\n",
        "# =============================================================================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        labels: List[int],\n",
        "        tokenizer,\n",
        "        max_len: int = 128,\n",
        "        mode: str = \"flat_chapter\",  # 'flat_chapter', 'flat_section', 'hierarchical'\n",
        "        subject_labels: List[int] = None\n",
        "    ):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.mode = mode\n",
        "        self.subject_labels = subject_labels  # 只有 hierarchical 模式才需要\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        if self.mode == \"hierarchical\":\n",
        "            item[\"subject_labels\"] = torch.tensor(self.subject_labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# =============================================================================\n",
        "# 4. TextCNN Model 定義\n",
        "# =============================================================================\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_classes: int,\n",
        "        kernel_sizes: List[int] = [3,4,5],\n",
        "        num_filters: int = 100,\n",
        "        dropout_p: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (B, L)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.embedding(input_ids)        # (B, L, D)\n",
        "        x = x.permute(0, 2, 1)               # (B, D, L)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            c = conv(x)                      # (B, F, L - k + 1)\n",
        "            c = torch.relu(c)                # (B, F, L - k + 1)\n",
        "            c = torch.max_pool1d(c, kernel_size=c.size(2))  # (B, F, 1)\n",
        "            conv_outs.append(c.squeeze(2))   # (B, F)\n",
        "        cat = torch.cat(conv_outs, dim=1)    # (B, F * len(kernel_sizes))\n",
        "        drop = self.dropout(cat)             # (B, F * len(kernel_sizes))\n",
        "        logits = self.fc(drop)               # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 基礎 MLP Model 定義 (只用於 flat 模式)\n",
        "# =============================================================================\n",
        "class BasicMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        features: (B, input_dim)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.fc1(features)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 訓練與評估函式\n",
        "# =============================================================================\n",
        "def compute_metrics(preds_and_labels) -> Dict[str, float]:\n",
        "    logits, labels = preds_and_labels\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_macro\": prec,\n",
        "        \"recall_macro\": rec,\n",
        "        \"f1_macro\": f1\n",
        "    }\n",
        "\n",
        "def train_flat_transformer(\n",
        "    model_name: str,\n",
        "    num_labels: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    output_dir: str,\n",
        "    device: torch.device,\n",
        "    use_dora: bool = False\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 flat (Subject+Chapter 或 flat Section) 模式的小型 Transformer (BERT/ RoBERTa)，回傳 metrics。\n",
        "    如果 use_dora=True，會在模型上套 DoRA Adapter（使用 LoraConfig with use_dora=True）。\n",
        "    \"\"\"\n",
        "    # 1. 選擇 tokenizer & base_model\n",
        "    if \"roberta\" in model_name:\n",
        "        tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    else:\n",
        "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    # 2. 如果要用 DoRA，包成 PEFT 模型\n",
        "    if use_dora:\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            inference_mode=False,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.05,\n",
        "            target_modules=[\"query\", \"value\"],  # 對 BERT/RoBERTa 模型，\"query\" 和 \"value\" 即可\n",
        "            use_dora=True,                      # 啟用 DoRA\n",
        "            #distillation_loss_weight=0.2,\n",
        "            #retention_loss_weight=0.1\n",
        "        )\n",
        "        model = get_peft_model(base_model, peft_config)\n",
        "    else:\n",
        "        model = base_model\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # 3. 構造 Dataset\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "\n",
        "    # 4. TrainingArguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=3e-5 if not use_dora else 3e-4,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        logging_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    # 5. 初始化 Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        #label_names=[str(i) for i in range(num_labels)] #新增\n",
        "    )\n",
        "\n",
        "    # 6. 訓練 & 驗證\n",
        "    start = time.time()\n",
        "    trainer.train()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    # 7. 儲存 adapter（若有 DoRA）或整個模型\n",
        "    #    如果 use_dora=True，Trainer.save_model() 會把 PEFT adapter 一併儲存到 output_dir\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # 8. 計算檔案大小\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    metrics[\"eval_accuracy\"],\n",
        "        \"precision\":   metrics[\"eval_precision_macro\"],\n",
        "        \"recall\":      metrics[\"eval_recall_macro\"],\n",
        "        \"f1\":          metrics[\"eval_f1_macro\"],\n",
        "        \"train_time\":  elapsed,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "def train_textcnn(\n",
        "    vocab_size: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    tokenizer,\n",
        "    num_labels: int,\n",
        "    output_dir: str,\n",
        "    device: torch.device\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 TextCNN (flat_chapter) 模式，回傳 metrics。\n",
        "    \"\"\"\n",
        "    max_len = 128\n",
        "    batch_size = 32\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    embed_dim = 300\n",
        "    model = TextCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    history = {\"train_loss\": [], \"valid_loss\": [], \"train_f1\": [], \"valid_f1\": []}\n",
        "\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "        _, _, train_f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        total_vloss = 0.0\n",
        "        v_preds, v_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                logits = model(input_ids)\n",
        "                loss = criterion(logits, labels)\n",
        "                total_vloss += loss.item() * input_ids.size(0)\n",
        "                preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "                v_preds.extend(preds)\n",
        "                v_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        valid_loss = total_vloss / len(valid_loader.dataset)\n",
        "        _, _, valid_f1, _ = precision_recall_fscore_support(\n",
        "            v_labels, v_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"valid_loss\"].append(valid_loss)\n",
        "        history[\"train_f1\"].append(train_f1)\n",
        "        history[\"valid_f1\"].append(valid_f1)\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if valid_f1 > best_f1:\n",
        "            best_f1 = valid_f1\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/best_textcnn.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss {train_loss:.4f} | Valid Loss {valid_loss:.4f} | \"\n",
        "              f\"Train F1 {train_f1:.4f} | Valid F1 {valid_f1:.4f}\")\n",
        "\n",
        "    # 存檔： tokenizer vocab\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    None,\n",
        "        \"precision\":   None,\n",
        "        \"recall\":      None,\n",
        "        \"f1\":          best_f1,\n",
        "        \"train_time\":  None,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Pipeline 主程序：整合上述所有 case\n",
        "# =============================================================================\n",
        "def main_pipeline():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. 載入資料與 Labels\n",
        "    df, subj_map, chap_map, sect_map = load_and_prepare([\"math\", \"science\"]) # social\n",
        "    label2id_subject, id2label_subject = subj_map\n",
        "    label2id_chapter, id2label_chapter = chap_map\n",
        "    label2id_section, id2label_section = sect_map\n",
        "\n",
        "    # 2. 切 train/valid/test（90/10 → 再分81/9/10)\n",
        "    base = df[[\"ques_detl\", \"label_subject\", \"label_chapter\", \"label_section\", \"subject\"]].copy()\n",
        "\n",
        "    # 先切出 10% 做最終 test\n",
        "    rest, test = train_test_split(\n",
        "        base, test_size=0.1, stratify=base[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 再把 rest 切成 90/10 ≈ 81%/9% 做 train/valid\n",
        "    train_df, valid_df = train_test_split(\n",
        "        rest, test_size=0.1, stratify=rest[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 取出 text 和各種 label lists\n",
        "    train_texts = train_df[\"ques_detl\"].tolist()\n",
        "    valid_texts = valid_df[\"ques_detl\"].tolist()\n",
        "    test_texts  = test[\"ques_detl\"].tolist()\n",
        "\n",
        "    train_subj_labels = train_df[\"label_subject\"].tolist()\n",
        "    valid_subj_labels = valid_df[\"label_subject\"].tolist()\n",
        "    test_subj_labels  = test[\"label_subject\"].tolist()\n",
        "\n",
        "    train_chap_labels = train_df[\"label_chapter\"].tolist()\n",
        "    valid_chap_labels = valid_df[\"label_chapter\"].tolist()\n",
        "    test_chap_labels  = test[\"label_chapter\"].tolist()\n",
        "\n",
        "    train_sect_labels = train_df[\"label_section\"].tolist()\n",
        "    valid_sect_labels = valid_df[\"label_section\"].tolist()\n",
        "    test_sect_labels  = test[\"label_section\"].tolist()\n",
        "\n",
        "    # 3. 建立 tokenizer (給所有 TextCNN/Transformer 共用)\n",
        "    bert_tok    = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "    roberta_tok = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    # 4. 建立詞彙大小 (TextCNN 用)\n",
        "    vocab_size = bert_tok.vocab_size\n",
        "\n",
        "    # 5. 實驗配置\n",
        "    model_types = [\n",
        "        #(\"DoRA_BERT\",    \"bert-base-uncased\", True),\n",
        "        (\"DoRA_RoBERTa\", \"roberta-base\",       True),\n",
        "        #(\"BERT\",         \"bert-base-uncased\", False),\n",
        "        #(\"RoBERTa\",      \"roberta-base\",       False),\n",
        "    ]\n",
        "    others = [\"TextCNN\", \"MLP\"]\n",
        "\n",
        "    # 6. 三種分類策略\n",
        "    strategies = [\n",
        "        \"flat_chapter\",          # 直接分類 subject+chapter\n",
        "        \"flat_section_then_map\", # 先分類 subject+chapter+section，再 map 回 chapter\n",
        "        \"hierarchical\"           # 先分 subject，再分 chapter\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_name, hf_model_name, use_dora in model_types:\n",
        "        for strat in strategies:\n",
        "            exp_name = f\"{model_name}__{strat}\"\n",
        "            output_dir = f\"./outputs/{exp_name}\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            print(f\"\\n▶開始實驗: {exp_name}\")\n",
        "\n",
        "            if strat == \"flat_chapter\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_chapter),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_chap_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_chap_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"flat_section_then_map\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_section),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_sect_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_sect_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                # 如需計算 chapter fine，須先批次推論再做 mapping\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"hierarchical\":\n",
        "                sub_output = os.path.join(output_dir, \"subject_model\")\n",
        "                chap_output = os.path.join(output_dir, \"chapter_model\")\n",
        "                os.makedirs(sub_output, exist_ok=True)\n",
        "                os.makedirs(chap_output, exist_ok=True)\n",
        "\n",
        "                # 1) 訓練 subject\n",
        "                sub_metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_subject),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_subj_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_subj_labels,\n",
        "                    output_dir=sub_output,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "\n",
        "                # 2) 針對每個 subject 訓練 chapter classifier\n",
        "                chap_metrics = {}\n",
        "                for subj_id, subj_name in id2label_subject.items():\n",
        "                    idx_train = [i for i, s in enumerate(train_subj_labels) if s == subj_id]\n",
        "                    idx_valid = [i for i, s in enumerate(valid_subj_labels) if s == subj_id]\n",
        "\n",
        "                    sub_train_texts = [train_texts[i] for i in idx_train]\n",
        "                    sub_train_chaps = [train_chap_labels[i] for i in idx_train]\n",
        "                    sub_valid_texts = [valid_texts[i] for i in idx_valid]\n",
        "                    sub_valid_chaps = [valid_chap_labels[i] for i in idx_valid]\n",
        "\n",
        "                    unique_chaps = sorted({train_chap_labels[i] for i in idx_train})\n",
        "                    if len(unique_chaps) < 2:\n",
        "                        continue\n",
        "\n",
        "                    sub_dir = os.path.join(chap_output, f\"subj_{subj_id}\")\n",
        "                    os.makedirs(sub_dir, exist_ok=True)\n",
        "                    sub_chap_m = train_flat_transformer(\n",
        "                        model_name=hf_model_name,\n",
        "                        num_labels=len(unique_chaps),\n",
        "                        train_texts=sub_train_texts,\n",
        "                        train_labels=sub_train_chaps,\n",
        "                        valid_texts=sub_valid_texts,\n",
        "                        valid_labels=sub_valid_chaps,\n",
        "                        output_dir=sub_dir,\n",
        "                        device=device,\n",
        "                        use_dora=use_dora\n",
        "                    )\n",
        "                    chap_metrics[subj_id] = sub_chap_m\n",
        "\n",
        "                results.append((exp_name, {\"subject\": sub_metrics, \"chapter\": chap_metrics}))\n",
        "\n",
        "    # TextCNN + MLP 只負責 flat_chapter\n",
        "    for model_name in others:\n",
        "        strat = \"flat_chapter\"\n",
        "        exp_name = f\"{model_name}__{strat}\"\n",
        "        output_dir = f\"./outputs/{exp_name}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"\\n▶開始實驗: {exp_name}\")\n",
        "\n",
        "        if model_name == \"TextCNN\":\n",
        "            start = time.time()\n",
        "            metrics = train_textcnn(\n",
        "                vocab_size=vocab_size,\n",
        "                train_texts=train_texts,\n",
        "                train_labels=train_chap_labels,\n",
        "                valid_texts=valid_texts,\n",
        "                valid_labels=valid_chap_labels,\n",
        "                tokenizer=bert_tok,\n",
        "                num_labels=len(label2id_chapter),\n",
        "                output_dir=output_dir,\n",
        "                device=device\n",
        "            )\n",
        "            elapsed = time.time() - start\n",
        "            metrics[\"train_time\"] = elapsed\n",
        "            results.append((exp_name, metrics))\n",
        "\n",
        "        elif model_name == \"MLP\":\n",
        "            train_enc = bert_tok(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            valid_enc = bert_tok(valid_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "            bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                \"bert-base-uncased\", output_hidden_states=True\n",
        "            )\n",
        "            bert_model.to(device)\n",
        "            bert_model.eval()\n",
        "\n",
        "            def encode_avg(inputs):\n",
        "                input_ids = inputs[\"input_ids\"].to(device)\n",
        "                attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "                with torch.no_grad():\n",
        "                    outputs = bert_model.bert(input_ids, attention_mask=attention_mask)\n",
        "                    last_hidden = outputs.last_hidden_state  # (B, L, D)\n",
        "                    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "                    summed = torch.sum(last_hidden * mask, 1)\n",
        "                    counts = torch.clamp(mask.sum(1), min=1e-9)\n",
        "                    avg_pooled = summed / counts\n",
        "                    return avg_pooled.cpu()\n",
        "\n",
        "            train_feats = encode_avg(train_enc)\n",
        "            valid_feats = encode_avg(valid_enc)\n",
        "\n",
        "            input_dim = train_feats.size(1)\n",
        "            hidden_dim = 256\n",
        "            model = BasicMLP(input_dim, hidden_dim, num_classes=len(label2id_chapter))\n",
        "            model.to(device)\n",
        "\n",
        "            train_labels_tensor = torch.tensor(train_chap_labels)\n",
        "            valid_labels_tensor = torch.tensor(valid_chap_labels)\n",
        "\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            start = time.time()\n",
        "            best_f1 = 0.0\n",
        "            for epoch in range(1, 6):\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(train_feats.to(device))\n",
        "                loss = criterion(logits, train_labels_tensor.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    v_logits = model(valid_feats.to(device))\n",
        "                    v_preds = torch.argmax(v_logits, dim=1).cpu().numpy()\n",
        "                    v_labels = valid_labels_tensor.numpy()\n",
        "                    _, _, v_f1, _ = precision_recall_fscore_support(v_labels, v_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "                if v_f1 > best_f1:\n",
        "                    best_f1 = v_f1\n",
        "                    torch.save(model.state_dict(), f\"{output_dir}/best_mlp.pt\")\n",
        "                print(f\"Epoch {epoch} | Valid F1 {v_f1:.4f}\")\n",
        "\n",
        "            elapsed = time.time() - start\n",
        "            total_size = 0\n",
        "            for root, _, files in os.walk(output_dir):\n",
        "                for fname in files:\n",
        "                    total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "            results.append((exp_name, {\n",
        "                \"accuracy\":    None,\n",
        "                \"precision\":   None,\n",
        "                \"recall\":      None,\n",
        "                \"f1\":          best_f1,\n",
        "                \"train_time\":  elapsed,\n",
        "                \"model_size\":  total_size\n",
        "            }))\n",
        "\n",
        "    # 8. 把所有實驗結果匯出\n",
        "    out_df = pd.DataFrame([{\"experiment\": name, **metrics} for name, metrics in results])\n",
        "    out_df.to_csv(\"experiment_results.csv\", index=False)\n",
        "    print(\"\\n所有實驗完成，結果已存到 experiment_results.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1JF34rsD2dQ3",
        "outputId": "b5908625-9967-4994-a6a5-af5476bb5e9e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗: DoRA_RoBERTa__flat_chapter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-7-675a3b50221a>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6736' max='6736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6736/6736 16:31, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.641500</td>\n",
              "      <td>1.824183</td>\n",
              "      <td>0.468938</td>\n",
              "      <td>0.425457</td>\n",
              "      <td>0.345104</td>\n",
              "      <td>0.331096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.606900</td>\n",
              "      <td>1.364425</td>\n",
              "      <td>0.596526</td>\n",
              "      <td>0.524921</td>\n",
              "      <td>0.474058</td>\n",
              "      <td>0.470217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.269700</td>\n",
              "      <td>1.174923</td>\n",
              "      <td>0.650635</td>\n",
              "      <td>0.570335</td>\n",
              "      <td>0.543957</td>\n",
              "      <td>0.534305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.087600</td>\n",
              "      <td>1.056894</td>\n",
              "      <td>0.680027</td>\n",
              "      <td>0.583057</td>\n",
              "      <td>0.568988</td>\n",
              "      <td>0.566207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.973900</td>\n",
              "      <td>1.013577</td>\n",
              "      <td>0.698063</td>\n",
              "      <td>0.642917</td>\n",
              "      <td>0.591359</td>\n",
              "      <td>0.596090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.887800</td>\n",
              "      <td>0.964561</td>\n",
              "      <td>0.711423</td>\n",
              "      <td>0.629691</td>\n",
              "      <td>0.609841</td>\n",
              "      <td>0.609068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.802400</td>\n",
              "      <td>0.932254</td>\n",
              "      <td>0.727455</td>\n",
              "      <td>0.644827</td>\n",
              "      <td>0.625918</td>\n",
              "      <td>0.628278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.756700</td>\n",
              "      <td>0.910026</td>\n",
              "      <td>0.728791</td>\n",
              "      <td>0.647678</td>\n",
              "      <td>0.629052</td>\n",
              "      <td>0.630642</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗: DoRA_RoBERTa__flat_section_then_map\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-7-675a3b50221a>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6736' max='6736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6736/6736 16:34, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.840300</td>\n",
              "      <td>3.161893</td>\n",
              "      <td>0.267201</td>\n",
              "      <td>0.153120</td>\n",
              "      <td>0.133818</td>\n",
              "      <td>0.118461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.922200</td>\n",
              "      <td>2.598913</td>\n",
              "      <td>0.382098</td>\n",
              "      <td>0.226270</td>\n",
              "      <td>0.210320</td>\n",
              "      <td>0.198403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.346200</td>\n",
              "      <td>2.030571</td>\n",
              "      <td>0.480294</td>\n",
              "      <td>0.332805</td>\n",
              "      <td>0.310361</td>\n",
              "      <td>0.291222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.904000</td>\n",
              "      <td>1.780325</td>\n",
              "      <td>0.535070</td>\n",
              "      <td>0.401491</td>\n",
              "      <td>0.391488</td>\n",
              "      <td>0.371050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.632000</td>\n",
              "      <td>1.605700</td>\n",
              "      <td>0.575150</td>\n",
              "      <td>0.449016</td>\n",
              "      <td>0.436769</td>\n",
              "      <td>0.414048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.466100</td>\n",
              "      <td>1.508339</td>\n",
              "      <td>0.590514</td>\n",
              "      <td>0.477029</td>\n",
              "      <td>0.458082</td>\n",
              "      <td>0.442144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.339600</td>\n",
              "      <td>1.451455</td>\n",
              "      <td>0.611222</td>\n",
              "      <td>0.503476</td>\n",
              "      <td>0.480708</td>\n",
              "      <td>0.463024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.256400</td>\n",
              "      <td>1.421235</td>\n",
              "      <td>0.621242</td>\n",
              "      <td>0.509115</td>\n",
              "      <td>0.490593</td>\n",
              "      <td>0.473420</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗: DoRA_RoBERTa__hierarchical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-7-675a3b50221a>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6736' max='6736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6736/6736 16:34, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.168200</td>\n",
              "      <td>0.097383</td>\n",
              "      <td>0.969940</td>\n",
              "      <td>0.966574</td>\n",
              "      <td>0.970724</td>\n",
              "      <td>0.968538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.082800</td>\n",
              "      <td>0.094500</td>\n",
              "      <td>0.970608</td>\n",
              "      <td>0.973685</td>\n",
              "      <td>0.964749</td>\n",
              "      <td>0.968814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.062300</td>\n",
              "      <td>0.062120</td>\n",
              "      <td>0.983300</td>\n",
              "      <td>0.983501</td>\n",
              "      <td>0.981354</td>\n",
              "      <td>0.982403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.045000</td>\n",
              "      <td>0.078380</td>\n",
              "      <td>0.983300</td>\n",
              "      <td>0.983188</td>\n",
              "      <td>0.981665</td>\n",
              "      <td>0.982414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.037600</td>\n",
              "      <td>0.093180</td>\n",
              "      <td>0.982632</td>\n",
              "      <td>0.983279</td>\n",
              "      <td>0.980186</td>\n",
              "      <td>0.981681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.032100</td>\n",
              "      <td>0.113129</td>\n",
              "      <td>0.979292</td>\n",
              "      <td>0.980927</td>\n",
              "      <td>0.975587</td>\n",
              "      <td>0.978109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.024200</td>\n",
              "      <td>0.113351</td>\n",
              "      <td>0.981296</td>\n",
              "      <td>0.982534</td>\n",
              "      <td>0.978160</td>\n",
              "      <td>0.980247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.023600</td>\n",
              "      <td>0.108515</td>\n",
              "      <td>0.982632</td>\n",
              "      <td>0.983279</td>\n",
              "      <td>0.980186</td>\n",
              "      <td>0.981681</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-7-675a3b50221a>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4104' max='4104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4104/4104 10:08, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.944900</td>\n",
              "      <td>1.268734</td>\n",
              "      <td>0.603939</td>\n",
              "      <td>0.629555</td>\n",
              "      <td>0.565879</td>\n",
              "      <td>0.557811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.172700</td>\n",
              "      <td>0.978309</td>\n",
              "      <td>0.683807</td>\n",
              "      <td>0.680773</td>\n",
              "      <td>0.652380</td>\n",
              "      <td>0.656122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.909600</td>\n",
              "      <td>0.811539</td>\n",
              "      <td>0.743982</td>\n",
              "      <td>0.736183</td>\n",
              "      <td>0.711048</td>\n",
              "      <td>0.718288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.759700</td>\n",
              "      <td>0.731603</td>\n",
              "      <td>0.758206</td>\n",
              "      <td>0.755476</td>\n",
              "      <td>0.729023</td>\n",
              "      <td>0.735986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.660700</td>\n",
              "      <td>0.668726</td>\n",
              "      <td>0.788840</td>\n",
              "      <td>0.781517</td>\n",
              "      <td>0.771958</td>\n",
              "      <td>0.771718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.587300</td>\n",
              "      <td>0.652965</td>\n",
              "      <td>0.805252</td>\n",
              "      <td>0.807286</td>\n",
              "      <td>0.791714</td>\n",
              "      <td>0.791014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.536900</td>\n",
              "      <td>0.626750</td>\n",
              "      <td>0.811816</td>\n",
              "      <td>0.814444</td>\n",
              "      <td>0.803152</td>\n",
              "      <td>0.806837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.506200</td>\n",
              "      <td>0.620007</td>\n",
              "      <td>0.811816</td>\n",
              "      <td>0.814242</td>\n",
              "      <td>0.796511</td>\n",
              "      <td>0.802098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29/29 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-7-675a3b50221a>:263: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-675a3b50221a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m     \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-675a3b50221a>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0msub_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchap_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"subj_{subj_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                     sub_chap_m = train_flat_transformer(\n\u001b[0m\u001b[1;32m    548\u001b[0m                         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                         \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_chaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-675a3b50221a>\u001b[0m in \u001b[0;36mtrain_flat_transformer\u001b[0;34m(model_name, num_labels, train_texts, train_labels, valid_texts, valid_labels, output_dir, device, use_dora)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;31m# 6. 訓練 & 驗證\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Dict, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType  # 只留 LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 固定亂數種子，確保結果可重現\n",
        "# =============================================================================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 資料載入與 Label Processing\n",
        "# =============================================================================\n",
        "def load_and_prepare(subjects: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    讀取多個 subject 的資料，合併後做 label encoding：\n",
        "      - label_subject\n",
        "      - label_chapter\n",
        "      - label_section\n",
        "    \"\"\"\n",
        "    def load_and_merge(subject: str) -> pd.DataFrame:\n",
        "        base_path = f\"{subject}_Database\"\n",
        "        qdf = pd.read_csv(f\"{base_path}/{subject}_question_bank.csv\")\n",
        "        cdf = pd.read_csv(f\"{base_path}/{subject}_chapter_list.csv\")\n",
        "        qdf.columns = qdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        cdf.columns = cdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        df = qdf.merge(cdf, on=\"section_name\", how=\"left\")\n",
        "        df[\"subject\"] = subject\n",
        "        return df\n",
        "\n",
        "    # 合併指定 subjects 的資料\n",
        "    df = pd.concat([load_and_merge(s) for s in subjects], ignore_index=True)\n",
        "    df = df[[\"subject\", \"chapter_name_x\", \"section_name\", \"ques_detl\"]].dropna().reset_index(drop=True)\n",
        "\n",
        "    # subject label\n",
        "    df[\"label_str\"] = df[\"subject\"]\n",
        "    label2id_subject = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_subject = {i: lab for lab, i in label2id_subject.items()}\n",
        "    df[\"label_subject\"] = df[\"label_str\"].map(label2id_subject)\n",
        "\n",
        "    # chapter label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"]\n",
        "    label2id_chapter = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_chapter = {i: lab for lab, i in label2id_chapter.items()}\n",
        "    df[\"label_chapter\"] = df[\"label_str\"].map(label2id_chapter)\n",
        "\n",
        "    # section label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"] + \"::\" + df[\"section_name\"]\n",
        "    label2id_section = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_section = {i: lab for lab, i in label2id_section.items()}\n",
        "    df[\"label_section\"] = df[\"label_str\"].map(label2id_section)\n",
        "\n",
        "    # 只保留至少出現兩次的 section\n",
        "    vc = df[\"label_section\"].value_counts()\n",
        "    valid_secs = set(vc[vc >= 2].index)\n",
        "    df = df[df[\"label_section\"].isin(valid_secs)].reset_index(drop=True)\n",
        "\n",
        "    return df, (label2id_subject, id2label_subject), (label2id_chapter, id2label_chapter), (label2id_section, id2label_section)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 自訂 Dataset\n",
        "# =============================================================================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        labels: List[int],\n",
        "        tokenizer,\n",
        "        max_len: int = 128,\n",
        "        mode: str = \"flat_chapter\",  # 'flat_chapter', 'flat_section', 'hierarchical'\n",
        "        subject_labels: List[int] = None\n",
        "    ):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.mode = mode\n",
        "        self.subject_labels = subject_labels  # 只有 hierarchical 模式才需要\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        if self.mode == \"hierarchical\":\n",
        "            item[\"subject_labels\"] = torch.tensor(self.subject_labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# =============================================================================\n",
        "# 4. TextCNN Model 定義\n",
        "# =============================================================================\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_classes: int,\n",
        "        kernel_sizes: List[int] = [3,4,5],\n",
        "        num_filters: int = 100,\n",
        "        dropout_p: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (B, L)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.embedding(input_ids)        # (B, L, D)\n",
        "        x = x.permute(0, 2, 1)               # (B, D, L)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            c = conv(x)                      # (B, F, L - k + 1)\n",
        "            c = torch.relu(c)                # (B, F, L - k + 1)\n",
        "            c = torch.max_pool1d(c, kernel_size=c.size(2))  # (B, F, 1)\n",
        "            conv_outs.append(c.squeeze(2))   # (B, F)\n",
        "        cat = torch.cat(conv_outs, dim=1)    # (B, F * len(kernel_sizes))\n",
        "        drop = self.dropout(cat)             # (B, F * len(kernel_sizes))\n",
        "        logits = self.fc(drop)               # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 基礎 MLP Model 定義 (只用於 flat 模式)\n",
        "# =============================================================================\n",
        "class BasicMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        features: (B, input_dim)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.fc1(features)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 訓練與評估函式\n",
        "# =============================================================================\n",
        "def compute_metrics(preds_and_labels) -> Dict[str, float]:\n",
        "    logits, labels = preds_and_labels\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "def train_flat_transformer(\n",
        "    model_name: str,\n",
        "    num_labels: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    output_dir: str,\n",
        "    device: torch.device,\n",
        "    use_dora: bool = False\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 flat (Chapter or Section) 模式的小型 Transformer (BERT/ RoBERTa)，回傳 metrics。\n",
        "    如果 use_dora=True，會在模型上套 DoRA Adapter（LoraConfig）。\n",
        "    \"\"\"\n",
        "    # 1. 選擇 tokenizer & base_model\n",
        "    if \"roberta\" in model_name:\n",
        "        tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    else:\n",
        "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    # 2. 如果要用 DoRA，包成 PEFT 模型\n",
        "    if use_dora:\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            inference_mode=False,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.05,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            use_dora=True,\n",
        "        )\n",
        "        model = get_peft_model(base_model, peft_config)\n",
        "    else:\n",
        "        model = base_model\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # 3. 構造 Dataset\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "\n",
        "    # 4. TrainingArguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=3e-5 if not use_dora else 3e-4,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        logging_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    # 5. 初始化 Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # 6. 訓練 & 驗證\n",
        "    start = time.time()\n",
        "    trainer.train()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    # 7. 儲存 adapter（若有 DoRA）或整個模型\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # 8. 計算檔案大小\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    metrics[\"eval_accuracy\"],\n",
        "        \"precision\":   metrics[\"eval_precision\"],\n",
        "        \"recall\":      metrics[\"eval_recall\"],\n",
        "        \"f1\":          metrics[\"eval_f1\"],\n",
        "        \"train_time\":  elapsed,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "def train_textcnn(\n",
        "    vocab_size: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    tokenizer,\n",
        "    num_labels: int,\n",
        "    output_dir: str,\n",
        "    device: torch.device\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 TextCNN (flat 模式)，回傳 metrics (accuracy, precision, recall, f1, train_time, model_size)。\n",
        "    \"\"\"\n",
        "    max_len = 128\n",
        "    batch_size = 32\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    embed_dim = 300\n",
        "    model = TextCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_metrics = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "        train_acc = accuracy_score(all_labels, all_preds)\n",
        "        train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        total_vloss = 0.0\n",
        "        v_preds, v_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                logits = model(input_ids)\n",
        "                loss = criterion(logits, labels)\n",
        "                total_vloss += loss.item() * input_ids.size(0)\n",
        "                preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "                v_preds.extend(preds)\n",
        "                v_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        valid_loss = total_vloss / len(valid_loader.dataset)\n",
        "        valid_acc = accuracy_score(v_labels, v_preds)\n",
        "        valid_prec, valid_rec, valid_f1, _ = precision_recall_fscore_support(\n",
        "            v_labels, v_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if valid_f1 > best_f1:\n",
        "            best_f1 = valid_f1\n",
        "            best_metrics = {\n",
        "                \"accuracy\":  valid_acc,\n",
        "                \"precision\": valid_prec,\n",
        "                \"recall\":    valid_rec,\n",
        "                \"f1\":        valid_f1\n",
        "            }\n",
        "            # 存模型前先確保 output_dir 存在\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/best_textcnn.pt\")\n",
        "\n",
        "        print(\n",
        "            f\"[TextCNN] Epoch {epoch} | \"\n",
        "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | \"\n",
        "            f\"Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f} | Valid F1: {valid_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    best_metrics[\"accuracy\"],\n",
        "        \"precision\":   best_metrics[\"precision\"],\n",
        "        \"recall\":      best_metrics[\"recall\"],\n",
        "        \"f1\":          best_metrics[\"f1\"],\n",
        "        \"train_time\":  elapsed,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Pipeline 主程序：整合上述所有 case\n",
        "# =============================================================================\n",
        "def main_pipeline():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. 載入資料與 Labels\n",
        "    df, subj_map, chap_map, sect_map = load_and_prepare([\"math\", \"science\"]) # 社會科改成合適的 subject\n",
        "    label2id_subject, id2label_subject = subj_map\n",
        "    label2id_chapter, id2label_chapter = chap_map\n",
        "    label2id_section, id2label_section = sect_map\n",
        "\n",
        "    # 2. 切 train/valid/test（90/10 → 再分81/9/10)\n",
        "    base = df[[\"ques_detl\", \"label_subject\", \"label_chapter\", \"label_section\", \"subject\"]].copy()\n",
        "\n",
        "    # 先切出 10% 做最終 test\n",
        "    rest, test = train_test_split(\n",
        "        base, test_size=0.1, stratify=base[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 再把 rest 切成 90/10 ≈ 81%/9% 做 train/valid\n",
        "    train_df, valid_df = train_test_split(\n",
        "        rest, test_size=0.1, stratify=rest[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 取出 text 和各種 label lists\n",
        "    train_texts = train_df[\"ques_detl\"].tolist()\n",
        "    valid_texts = valid_df[\"ques_detl\"].tolist()\n",
        "    test_texts  = test[\"ques_detl\"].tolist()\n",
        "\n",
        "    train_subj_labels = train_df[\"label_subject\"].tolist()\n",
        "    valid_subj_labels = valid_df[\"label_subject\"].tolist()\n",
        "    test_subj_labels  = test[\"label_subject\"].tolist()\n",
        "\n",
        "    train_chap_labels = train_df[\"label_chapter\"].tolist()\n",
        "    valid_chap_labels = valid_df[\"label_chapter\"].tolist()\n",
        "    test_chap_labels  = test[\"label_chapter\"].tolist()\n",
        "\n",
        "    train_sect_labels = train_df[\"label_section\"].tolist()\n",
        "    valid_sect_labels = valid_df[\"label_section\"].tolist()\n",
        "    test_sect_labels  = test[\"label_section\"].tolist()\n",
        "\n",
        "    # 3. 建立 tokenizer (給所有 TextCNN/Transformer 共用)\n",
        "    bert_tok    = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "    roberta_tok = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    # 4. 建立詞彙大小 (TextCNN 用)\n",
        "    vocab_size = bert_tok.vocab_size\n",
        "\n",
        "    # 5. 實驗配置\n",
        "    model_types = [\n",
        "        # 範例：(\"BERT\", \"bert-base-uncased\", False),\n",
        "        #        (\"RoBERTa\", \"roberta-base\", False),\n",
        "        #        (\"DoRA_BERT\", \"bert-base-uncased\", True),\n",
        "        #        (\"DoRA_RoBERTa\", \"roberta-base\", True),\n",
        "    ]\n",
        "    others = [\"TextCNN\", \"MLP\"]\n",
        "\n",
        "    # 6. 三種分類策略\n",
        "    strategies = [\n",
        "        \"flat_chapter\",          # 直接分類 subject+chapter\n",
        "        \"flat_section\",          # 直接分類 subject+chapter+section\n",
        "        \"hierarchical\"           # 先分 subject，再分 chapter\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # (A) 先跑 Transformer 部分 (示例)\n",
        "    for model_name, hf_model_name, use_dora in model_types:\n",
        "        for strat in strategies:\n",
        "            exp_name = f\"{model_name}__{strat}\"\n",
        "            output_dir = f\"./outputs/{exp_name}\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            print(f\"\\n▶開始實驗 (Transformer): {exp_name}\")\n",
        "\n",
        "            if strat == \"flat_chapter\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_chapter),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_chap_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_chap_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"flat_section\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_section),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_sect_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_sect_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"hierarchical\":\n",
        "                # hierarchical: 先訓練 subject，再分 subj_id 針對 chapter\n",
        "                sub_output = os.path.join(output_dir, \"subj_model\")\n",
        "                chap_output = os.path.join(output_dir, \"chapter_model\")\n",
        "                os.makedirs(sub_output, exist_ok=True)\n",
        "                os.makedirs(chap_output, exist_ok=True)\n",
        "\n",
        "                print(\"  [Hierarchical] Step 1: 訓練 subject 模型\")\n",
        "                sub_metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_subject),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_subj_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_subj_labels,\n",
        "                    output_dir=sub_output,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "\n",
        "                chap_metrics = {}\n",
        "                print(\"  [Hierarchical] Step 2: 針對每個 subject 訓練 chapter 分類器\")\n",
        "                for subj_id, subj_name in id2label_subject.items():\n",
        "                    idx_train = [i for i, s in enumerate(train_subj_labels) if s == subj_id]\n",
        "                    idx_valid = [i for i, s in enumerate(valid_subj_labels) if s == subj_id]\n",
        "\n",
        "                    sub_train_texts = [train_texts[i] for i in idx_train]\n",
        "                    sub_train_chaps = [train_chap_labels[i] for i in idx_train]\n",
        "                    sub_valid_texts = [valid_texts[i] for i in idx_valid]\n",
        "                    sub_valid_chaps = [valid_chap_labels[i] for i in idx_valid]\n",
        "\n",
        "                    unique_chaps = sorted({train_chap_labels[i] for i in idx_train})\n",
        "                    if len(unique_chaps) < 2:\n",
        "                        continue\n",
        "\n",
        "                    sub_dir = os.path.join(chap_output, f\"subj_{subj_id}\")\n",
        "                    os.makedirs(sub_dir, exist_ok=True)\n",
        "\n",
        "                    print(f\"    [Subject {subj_id}] Training chapter classifier (num_labels={len(unique_chaps)})\")\n",
        "                    sub_chap_m = train_flat_transformer(\n",
        "                        model_name=hf_model_name,\n",
        "                        num_labels=len(unique_chaps),\n",
        "                        train_texts=sub_train_texts,\n",
        "                        train_labels=sub_train_chaps,\n",
        "                        valid_texts=sub_valid_texts,\n",
        "                        valid_labels=sub_valid_chaps,\n",
        "                        output_dir=sub_dir,\n",
        "                        device=device,\n",
        "                        use_dora=use_dora\n",
        "                    )\n",
        "                    chap_metrics[subj_id] = sub_chap_m\n",
        "\n",
        "                results.append((exp_name, {\"subject_metrics\": sub_metrics, \"chapter_metrics\": chap_metrics}))\n",
        "\n",
        "    # =============================================================================\n",
        "    # (B) TextCNN + MLP 只負責三種策略 (flat_chapter, flat_section, hierarchical)\n",
        "    # =============================================================================\n",
        "    for model_name in others:\n",
        "        for strat in strategies:\n",
        "            exp_name = f\"{model_name}__{strat}\"\n",
        "            output_dir = f\"./outputs/{exp_name}\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            print(f\"\\n▶開始實驗 (TextCNN/MLP): {exp_name}\")\n",
        "\n",
        "            # 如果是 hierarchical，先在這裡幫它建好 subdir\n",
        "            if strat == \"hierarchical\":\n",
        "                os.makedirs(os.path.join(output_dir, \"subj_model\"), exist_ok=True)\n",
        "                os.makedirs(os.path.join(output_dir, \"chapter_model\"), exist_ok=True)\n",
        "\n",
        "            if strat == \"flat_chapter\":\n",
        "                cur_train_labels = train_chap_labels\n",
        "                cur_valid_labels = valid_chap_labels\n",
        "                num_labels = len(label2id_chapter)\n",
        "\n",
        "            elif strat == \"flat_section\":\n",
        "                cur_train_labels = train_sect_labels\n",
        "                cur_valid_labels = valid_sect_labels\n",
        "                num_labels = len(label2id_section)\n",
        "\n",
        "            elif strat == \"hierarchical\":\n",
        "                cur_results = {\"subject\": {}, \"chapter\": {}}\n",
        "\n",
        "                # Step1: 訓練 subject 分類器\n",
        "                print(\"  [Hierarchical] Step1: 訓練 subject 分類器\")\n",
        "                # 先確保 subj_model 存在\n",
        "                subj_model_dir = os.path.join(output_dir, \"subj_model\")\n",
        "                os.makedirs(subj_model_dir, exist_ok=True)\n",
        "\n",
        "                if model_name == \"TextCNN\":\n",
        "                    # 用 chapter labels 當作 proxy 來當作 subject 分類（TextCNN 只支援 flat_chapter）\n",
        "                    sub_metrics = train_textcnn(\n",
        "                        vocab_size=vocab_size,\n",
        "                        train_texts=train_texts,\n",
        "                        train_labels=train_subj_labels,\n",
        "                        valid_texts=valid_texts,\n",
        "                        valid_labels=valid_subj_labels,\n",
        "                        tokenizer=bert_tok,\n",
        "                        num_labels=len(label2id_subject),\n",
        "                        output_dir=subj_model_dir,\n",
        "                        device=device\n",
        "                    )\n",
        "                else:  # MLP 版 subject\n",
        "                    enc_train = bert_tok(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "                    enc_valid = bert_tok(valid_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "                    bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                        \"bert-base-uncased\", output_hidden_states=True\n",
        "                    ).to(device)\n",
        "                    bert_model.eval()\n",
        "\n",
        "                    def encode_avg(inputs, batch_size=32):\n",
        "                        input_ids = inputs[\"input_ids\"]\n",
        "                        attention_mask = inputs[\"attention_mask\"]\n",
        "                        feats = []\n",
        "                        with torch.no_grad():\n",
        "                            for i in range(0, input_ids.size(0), batch_size):\n",
        "                                end_i = min(i + batch_size, input_ids.size(0))\n",
        "                                b_ids = input_ids[i:end_i].to(device)\n",
        "                                b_att = attention_mask[i:end_i].to(device)\n",
        "                                outs = bert_model.bert(b_ids, attention_mask=b_att).last_hidden_state\n",
        "                                mask = b_att.unsqueeze(-1).expand_as(outs).float()\n",
        "                                summed = torch.sum(outs * mask, dim=1)\n",
        "                                counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "                                avg = (summed / counts).cpu()\n",
        "                                feats.append(avg)\n",
        "                                del b_ids, b_att, outs, mask, summed, counts\n",
        "                                torch.cuda.empty_cache()\n",
        "                        return torch.cat(feats, dim=0)\n",
        "\n",
        "                    subj_train_feats = encode_avg(enc_train)\n",
        "                    subj_valid_feats = encode_avg(enc_valid)\n",
        "\n",
        "                    subj_model = BasicMLP(subj_train_feats.size(1), 256, len(label2id_subject)).to(device)\n",
        "                    subj_train_ds = TensorDataset(subj_train_feats, torch.tensor(train_subj_labels))\n",
        "                    subj_valid_ds = TensorDataset(subj_valid_feats, torch.tensor(valid_subj_labels))\n",
        "                    subj_train_loader = DataLoader(subj_train_ds, batch_size=64, shuffle=True)\n",
        "                    subj_valid_loader = DataLoader(subj_valid_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "                    opt = optim.Adam(subj_model.parameters(), lr=1e-4)\n",
        "                    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "                    best_f1_s = 0.0\n",
        "                    best_metrics_s = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "                    st = time.time()\n",
        "                    for epoch in range(1, 6):\n",
        "                        subj_model.train()\n",
        "                        train_preds, train_labels_acc = [], []\n",
        "                        run_loss = 0.0\n",
        "                        for xb, yb in subj_train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            opt.zero_grad()\n",
        "                            logits = subj_model(xb)\n",
        "                            loss = crit(logits, yb)\n",
        "                            loss.backward()\n",
        "                            opt.step()\n",
        "                            run_loss += loss.item() * xb.size(0)\n",
        "                            pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                            train_preds.extend(pr)\n",
        "                            train_labels_acc.extend(yb.cpu().numpy())\n",
        "\n",
        "                        train_acc_s = accuracy_score(train_labels_acc, train_preds)\n",
        "                        _, _, train_f1_s, _ = precision_recall_fscore_support(\n",
        "                            train_labels_acc, train_preds, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "\n",
        "                        subj_model.eval()\n",
        "                        val_preds_s, val_labels_s = [], []\n",
        "                        vloss_s = 0.0\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in subj_valid_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                logits = subj_model(xb)\n",
        "                                loss = crit(logits, yb)\n",
        "                                vloss_s += loss.item() * xb.size(0)\n",
        "                                pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                val_preds_s.extend(pr)\n",
        "                                val_labels_s.extend(yb.cpu().numpy())\n",
        "\n",
        "                        valid_acc_s = accuracy_score(val_labels_s, val_preds_s)\n",
        "                        _, _, valid_f1_s, _ = precision_recall_fscore_support(\n",
        "                            val_labels_s, val_preds_s, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "\n",
        "                        if valid_f1_s > best_f1_s:\n",
        "                            best_f1_s = valid_f1_s\n",
        "                            best_metrics_s = {\n",
        "                                \"accuracy\":  valid_acc_s,\n",
        "                                \"precision\": precision_score(val_labels_s, val_preds_s, average=\"macro\", zero_division=0),\n",
        "                                \"recall\":    recall_score(val_labels_s, val_preds_s, average=\"macro\", zero_division=0),\n",
        "                                \"f1\":        valid_f1_s\n",
        "                            }\n",
        "                            # 確保子目錄存在\n",
        "                            save_dir_s = os.path.join(output_dir, \"subj_model\")\n",
        "                            os.makedirs(save_dir_s, exist_ok=True)\n",
        "                            torch.save(subj_model.state_dict(), os.path.join(save_dir_s, \"best_subj_mlp.pt\"))\n",
        "\n",
        "                        print(\n",
        "                            f\"    [Subject MLP Epoch {epoch}] \"\n",
        "                            f\"Train Acc: {train_acc_s:.4f} | Train F1: {train_f1_s:.4f} | \"\n",
        "                            f\"Valid Acc: {valid_acc_s:.4f} | Valid F1: {valid_f1_s:.4f}\"\n",
        "                        )\n",
        "\n",
        "                    elapsed_s = time.time() - st\n",
        "                    total_sz_s = 0\n",
        "                    save_dir_s = os.path.join(output_dir, \"subj_model\")\n",
        "                    for root, _, files in os.walk(save_dir_s):\n",
        "                        for fname in files:\n",
        "                            total_sz_s += os.path.getsize(os.path.join(root, fname))\n",
        "                    sub_metrics = {\n",
        "                        \"accuracy\":    best_metrics_s[\"accuracy\"],\n",
        "                        \"precision\":   best_metrics_s[\"precision\"],\n",
        "                        \"recall\":      best_metrics_s[\"recall\"],\n",
        "                        \"f1\":          best_metrics_s[\"f1\"],\n",
        "                        \"train_time\":  elapsed_s,\n",
        "                        \"model_size\":  total_sz_s\n",
        "                    }\n",
        "\n",
        "                cur_results[\"subject\"][model_name] = sub_metrics  # TextCNN 或 MLP 的 subject 部分\n",
        "\n",
        "                # Step2: 針對每個 subject 訓練 chapter 分類器\n",
        "                print(\"  [Hierarchical] Step2: 針對每個 subject 訓練 chapter 分類器\")\n",
        "                for subj_id, subj_name in id2label_subject.items():\n",
        "                    idx_train = [i for i, s in enumerate(train_subj_labels) if s == subj_id]\n",
        "                    idx_valid = [i for i, s in enumerate(valid_subj_labels) if s == subj_id]\n",
        "\n",
        "                    sub_train_texts = [train_texts[i] for i in idx_train]\n",
        "                    sub_train_chaps = [train_chap_labels[i] for i in idx_train]\n",
        "                    sub_valid_texts = [valid_texts[i] for i in idx_valid]\n",
        "                    sub_valid_chaps = [valid_chap_labels[i] for i in idx_valid]\n",
        "\n",
        "                    unique_chaps = sorted({train_chap_labels[i] for i in idx_train})\n",
        "                    if len(unique_chaps) < 2:\n",
        "                        continue\n",
        "\n",
        "                    chap_dir = os.path.join(output_dir, \"chapter_model\", f\"subj_{subj_id}\")\n",
        "                    os.makedirs(chap_dir, exist_ok=True)\n",
        "\n",
        "                    if model_name == \"TextCNN\":\n",
        "                        chap_m = train_textcnn(\n",
        "                            vocab_size=vocab_size,\n",
        "                            train_texts=sub_train_texts,\n",
        "                            train_labels=sub_train_chaps,\n",
        "                            valid_texts=sub_valid_texts,\n",
        "                            valid_labels=sub_valid_chaps,\n",
        "                            tokenizer=bert_tok,\n",
        "                            num_labels=len(unique_chaps),\n",
        "                            output_dir=chap_dir,\n",
        "                            device=device\n",
        "                        )\n",
        "                    else:  # MLP\n",
        "                        enc_tr = bert_tok(sub_train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "                        enc_vd = bert_tok(sub_valid_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "                        # 載入 BERT 作特徵擷取\n",
        "                        bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                            \"bert-base-uncased\", output_hidden_states=True\n",
        "                        ).to(device)\n",
        "                        bert_model.eval()\n",
        "\n",
        "                        def encode_avg(inputs, batch_size=32):\n",
        "                            input_ids = inputs[\"input_ids\"]\n",
        "                            attention_mask = inputs[\"attention_mask\"]\n",
        "                            feats = []\n",
        "                            with torch.no_grad():\n",
        "                                for i in range(0, input_ids.size(0), batch_size):\n",
        "                                    end_i = min(i + batch_size, input_ids.size(0))\n",
        "                                    b_ids = input_ids[i:end_i].to(device)\n",
        "                                    b_att = attention_mask[i:end_i].to(device)\n",
        "                                    outs = bert_model.bert(b_ids, attention_mask=b_att).last_hidden_state\n",
        "                                    mask = b_att.unsqueeze(-1).expand_as(outs).float()\n",
        "                                    summed = torch.sum(outs * mask, dim=1)\n",
        "                                    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "                                    avg = (summed / counts).cpu()\n",
        "                                    feats.append(avg)\n",
        "                                    del b_ids, b_att, outs, mask, summed, counts\n",
        "                                    torch.cuda.empty_cache()\n",
        "                            return torch.cat(feats, dim=0)\n",
        "\n",
        "                        tr_feats = encode_avg(enc_tr)\n",
        "                        vd_feats = encode_avg(enc_vd)\n",
        "\n",
        "                        mlp_model = BasicMLP(tr_feats.size(1), 256, len(unique_chaps)).to(device)\n",
        "                        tr_ds = TensorDataset(tr_feats, torch.tensor(sub_train_chaps))\n",
        "                        vd_ds = TensorDataset(vd_feats, torch.tensor(sub_valid_chaps))\n",
        "                        tr_loader = DataLoader(tr_ds, batch_size=64, shuffle=True)\n",
        "                        vd_loader = DataLoader(vd_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "                        opt_mc = optim.Adam(mlp_model.parameters(), lr=1e-4)\n",
        "                        crit_mc = nn.CrossEntropyLoss()\n",
        "\n",
        "                        best_f1_c = 0.0\n",
        "                        best_metrics_c = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "                        st_c = time.time()\n",
        "                        for epoch in range(1, 6):\n",
        "                            mlp_model.train()\n",
        "                            preds_tr, labels_tr = [], []\n",
        "                            runloss = 0.0\n",
        "                            for xb, yb in tr_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                opt_mc.zero_grad()\n",
        "                                logits = mlp_model(xb)\n",
        "                                loss = crit_mc(logits, yb)\n",
        "                                loss.backward()\n",
        "                                opt_mc.step()\n",
        "                                runloss += loss.item() * xb.size(0)\n",
        "                                pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                preds_tr.extend(pr)\n",
        "                                labels_tr.extend(yb.cpu().numpy())\n",
        "                            acc_tr_c = accuracy_score(labels_tr, preds_tr)\n",
        "                            _, _, f1_tr_c, _ = precision_recall_fscore_support(\n",
        "                                labels_tr, preds_tr, average=\"macro\", zero_division=0\n",
        "                            )\n",
        "\n",
        "                            mlp_model.eval()\n",
        "                            preds_vd, labels_vd = [], []\n",
        "                            vl = 0.0\n",
        "                            with torch.no_grad():\n",
        "                                for xb, yb in vd_loader:\n",
        "                                    xb, yb = xb.to(device), yb.to(device)\n",
        "                                    logits = mlp_model(xb)\n",
        "                                    loss = crit_mc(logits, yb)\n",
        "                                    vl += loss.item() * xb.size(0)\n",
        "                                    pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                    preds_vd.extend(pr)\n",
        "                                    labels_vd.extend(yb.cpu().numpy())\n",
        "                            acc_vd_c = accuracy_score(labels_vd, preds_vd)\n",
        "                            _, _, f1_vd_c, _ = precision_recall_fscore_support(\n",
        "                                labels_vd, preds_vd, average=\"macro\", zero_division=0\n",
        "                            )\n",
        "                            if f1_vd_c > best_f1_c:\n",
        "                                best_f1_c = f1_vd_c\n",
        "                                best_metrics_c = {\n",
        "                                    \"accuracy\":  acc_vd_c,\n",
        "                                    \"precision\": precision_score(labels_vd, preds_vd, average=\"macro\", zero_division=0),\n",
        "                                    \"recall\":    recall_score(labels_vd, preds_vd, average=\"macro\", zero_division=0),\n",
        "                                    \"f1\":        f1_vd_c\n",
        "                                }\n",
        "                                # 確保 chap_dir 存在\n",
        "                                os.makedirs(chap_dir, exist_ok=True)\n",
        "                                torch.save(mlp_model.state_dict(), os.path.join(chap_dir, \"best_chap_mlp.pt\"))\n",
        "\n",
        "                            print(\n",
        "                                f\"    [Chapter MLP subj_{subj_id} Epoch {epoch}] \"\n",
        "                                f\"Train Acc: {acc_tr_c:.4f} | Train F1: {f1_tr_c:.4f} | \"\n",
        "                                f\"Valid Acc: {acc_vd_c:.4f} | Valid F1: {f1_vd_c:.4f}\"\n",
        "                            )\n",
        "\n",
        "                        elapsed_c = time.time() - st_c\n",
        "                        total_sz_c = 0\n",
        "                        for root, _, files in os.walk(chap_dir):\n",
        "                            for fname in files:\n",
        "                                total_sz_c += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "                        chap_m = {\n",
        "                            \"accuracy\":    best_metrics_c[\"accuracy\"],\n",
        "                            \"precision\":   best_metrics_c[\"precision\"],\n",
        "                            \"recall\":      best_metrics_c[\"recall\"],\n",
        "                            \"f1\":          best_metrics_c[\"f1\"],\n",
        "                            \"train_time\":  elapsed_c,\n",
        "                            \"model_size\":  total_sz_c\n",
        "                        }\n",
        "\n",
        "                    cur_results[\"chapter\"][subj_id] = chap_m\n",
        "\n",
        "                results.append((exp_name, cur_results))\n",
        "                continue  # 跳過後面的 flat 處理\n",
        "\n",
        "            # 以下為 flat_chapter 或 flat_section 的 TextCNN/MLP 處理\n",
        "            if strat in [\"flat_chapter\", \"flat_section\"]:\n",
        "                if strat == \"flat_chapter\":\n",
        "                    cur_train_labels = train_chap_labels\n",
        "                    cur_valid_labels = valid_chap_labels\n",
        "                    num_labels = len(label2id_chapter)\n",
        "                else:  # flat_section\n",
        "                    cur_train_labels = train_sect_labels\n",
        "                    cur_valid_labels = valid_sect_labels\n",
        "                    num_labels = len(label2id_section)\n",
        "\n",
        "                if model_name == \"TextCNN\":\n",
        "                    print(\"  [TextCNN] Step1: 開始訓練\")\n",
        "                    start = time.time()\n",
        "\n",
        "                    metrics = train_textcnn(\n",
        "                        vocab_size=vocab_size,\n",
        "                        train_texts=train_texts,\n",
        "                        train_labels=cur_train_labels,\n",
        "                        valid_texts=valid_texts,\n",
        "                        valid_labels=cur_valid_labels,\n",
        "                        tokenizer=bert_tok,\n",
        "                        num_labels=num_labels,\n",
        "                        output_dir=output_dir,\n",
        "                        device=device\n",
        "                    )\n",
        "                    elapsed = time.time() - start\n",
        "                    metrics[\"train_time\"] = elapsed\n",
        "\n",
        "                    print(f\"    → accuracy:  {metrics['accuracy']:.4f}\")\n",
        "                    print(f\"    → precision: {metrics['precision']:.4f}\")\n",
        "                    print(f\"    → recall:    {metrics['recall']:.4f}\")\n",
        "                    print(f\"    → f1:        {metrics['f1']:.4f}\")\n",
        "                    print(f\"    → model_size: {metrics['model_size']} bytes\")\n",
        "\n",
        "                    results.append((exp_name, metrics))\n",
        "\n",
        "                else:  # MLP\n",
        "                    print(\"  [MLP] Step1: 用 tokenizer 編碼到 CPU\")\n",
        "                    train_enc = bert_tok(\n",
        "                        train_texts,\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    valid_enc = bert_tok(\n",
        "                        valid_texts,\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    print(f\"    → train_enc.shape: {train_enc['input_ids'].shape}\")\n",
        "                    print(f\"    → valid_enc.shape: {train_enc['input_ids'].shape}\")\n",
        "\n",
        "                    print(\"  [MLP] Step2: 載入 BERT 做特徵擷取\")\n",
        "                    bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                        \"bert-base-uncased\", output_hidden_states=True\n",
        "                    ).to(device)\n",
        "                    bert_model.eval()\n",
        "\n",
        "                    def encode_avg_dataset(inputs, batch_size: int = 32):\n",
        "                        input_ids = inputs[\"input_ids\"]\n",
        "                        attention_mask = inputs[\"attention_mask\"]\n",
        "                        features_list = []\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            for i in range(0, input_ids.size(0), batch_size):\n",
        "                                end = min(i + batch_size, input_ids.size(0))\n",
        "                                b_ids = input_ids[i:end].to(device)\n",
        "                                b_att = attention_mask[i:end].to(device)\n",
        "                                outs = bert_model.bert(b_ids, attention_mask=b_att).last_hidden_state\n",
        "                                mask = b_att.unsqueeze(-1).expand_as(outs).float()\n",
        "                                summed = torch.sum(outs * mask, dim=1)\n",
        "                                counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "                                avg = (summed / counts).cpu()\n",
        "                                features_list.append(avg)\n",
        "                                del b_ids, b_att, outs, mask, summed, counts\n",
        "                                torch.cuda.empty_cache()\n",
        "                        return torch.cat(features_list, dim=0)\n",
        "\n",
        "                    print(\"  [MLP] Step3: 計算 train_feats\")\n",
        "                    train_feats = encode_avg_dataset(train_enc, batch_size=32)\n",
        "                    print(f\"    → train_feats.shape = {train_feats.shape}\")\n",
        "\n",
        "                    print(\"  [MLP] Step4: 計算 valid_feats\")\n",
        "                    valid_feats = encode_avg_dataset(valid_enc, batch_size=32)\n",
        "                    print(f\"    → valid_feats.shape = {valid_feats.shape}\")\n",
        "\n",
        "                    print(\"  [MLP] Step5: 建立 MLP 並訓練\")\n",
        "                    input_dim = train_feats.size(1)\n",
        "                    hidden_dim = 256\n",
        "                    model = BasicMLP(input_dim, hidden_dim, num_labels).to(device)\n",
        "\n",
        "                    train_labels_tensor = torch.tensor(cur_train_labels, dtype=torch.long)\n",
        "                    valid_labels_tensor = torch.tensor(cur_valid_labels, dtype=torch.long)\n",
        "\n",
        "                    train_ds = TensorDataset(train_feats, train_labels_tensor)\n",
        "                    valid_ds = TensorDataset(valid_feats, valid_labels_tensor)\n",
        "                    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "                    valid_loader = DataLoader(valid_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    start = time.time()\n",
        "                    best_f1 = 0.0\n",
        "                    best_metrics = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "                    for epoch in range(1, 6):\n",
        "                        model.train()\n",
        "                        epoch_loss = 0.0\n",
        "                        train_preds, train_labels_acc = [], []\n",
        "\n",
        "                        for feats_batch, labels_batch in train_loader:\n",
        "                            feats_batch = feats_batch.to(device)\n",
        "                            labels_batch = labels_batch.to(device)\n",
        "\n",
        "                            optimizer.zero_grad()\n",
        "                            logits = model(feats_batch)\n",
        "                            loss = criterion(logits, labels_batch)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            epoch_loss += loss.item() * feats_batch.size(0)\n",
        "                            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                            train_preds.extend(preds)\n",
        "                            train_labels_acc.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "                        train_acc = accuracy_score(train_labels_acc, train_preds)\n",
        "                        train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
        "                            train_labels_acc, train_preds, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "                        avg_train_loss = epoch_loss / len(train_loader.dataset)\n",
        "\n",
        "                        model.eval()\n",
        "                        valid_preds, valid_labels_acc = [], []\n",
        "                        vloss = 0.0\n",
        "                        with torch.no_grad():\n",
        "                            for feats_batch, labels_batch in valid_loader:\n",
        "                                feats_batch = feats_batch.to(device)\n",
        "                                labels_batch = labels_batch.to(device)\n",
        "                                logits = model(feats_batch)\n",
        "                                loss = criterion(logits, labels_batch)\n",
        "                                vloss += loss.item() * feats_batch.size(0)\n",
        "                                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                valid_preds.extend(preds)\n",
        "                                valid_labels_acc.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "                        valid_acc = accuracy_score(valid_labels_acc, valid_preds)\n",
        "                        valid_prec, valid_rec, valid_f1, _ = precision_recall_fscore_support(\n",
        "                            valid_labels_acc, valid_preds, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "\n",
        "                        if valid_f1 > best_f1:\n",
        "                            best_f1 = valid_f1\n",
        "                            best_metrics = {\n",
        "                                \"accuracy\":  valid_acc,\n",
        "                                \"precision\": valid_prec,\n",
        "                                \"recall\":    valid_rec,\n",
        "                                \"f1\":        valid_f1\n",
        "                            }\n",
        "                            # 確保 output_dir 存在\n",
        "                            os.makedirs(output_dir, exist_ok=True)\n",
        "                            torch.save(model.state_dict(), f\"{output_dir}/best_mlp.pt\")\n",
        "\n",
        "                        print(\n",
        "                            f\"[MLP Epoch {epoch}] \"\n",
        "                            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "                            f\"Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | \"\n",
        "                            f\"Valid Acc: {valid_acc:.4f} | Valid F1: {valid_f1:.4f}\"\n",
        "                        )\n",
        "\n",
        "                    elapsed = time.time() - start\n",
        "                    total_size = 0\n",
        "                    for root, _, files in os.walk(output_dir):\n",
        "                        for fname in files:\n",
        "                            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "                    best_metrics[\"train_time\"] = elapsed\n",
        "                    best_metrics[\"model_size\"] = total_size\n",
        "                    print(f\"    → accuracy:  {best_metrics['accuracy']:.4f}\")\n",
        "                    print(f\"    → precision: {best_metrics['precision']:.4f}\")\n",
        "                    print(f\"    → recall:    {best_metrics['recall']:.4f}\")\n",
        "                    print(f\"    → f1:        {best_metrics['f1']:.4f}\")\n",
        "                    print(f\"    → model_size: {best_metrics['model_size']} bytes\")\n",
        "\n",
        "                    results.append((exp_name, best_metrics))\n",
        "\n",
        "    # =============================================================================\n",
        "    # 8. 把所有實驗結果匯出\n",
        "    # =============================================================================\n",
        "    out_df = pd.DataFrame([{\"experiment\": name, **metrics} for name, metrics in results])\n",
        "    out_df.to_csv(\"experiment_results.csv\", index=False)\n",
        "    print(\"\\n所有實驗完成，結果已存到 experiment_results.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VymDLayNTP_S",
        "outputId": "5cf49981-fe0d-4c11-e7a4-2ec4a4a8a88d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗 (TextCNN/MLP): TextCNN__flat_chapter\n",
            "  [TextCNN] Step1: 開始訓練\n",
            "[TextCNN] Epoch 1 | Train Loss: 3.3066 | Train Acc: 0.1835 | Train F1: 0.0907 | Valid Loss: 2.5390 | Valid Acc: 0.3721 | Valid F1: 0.2135\n",
            "[TextCNN] Epoch 2 | Train Loss: 2.4418 | Train Acc: 0.3646 | Train F1: 0.2351 | Valid Loss: 2.0105 | Valid Acc: 0.4830 | Valid F1: 0.3436\n",
            "[TextCNN] Epoch 3 | Train Loss: 2.0279 | Train Acc: 0.4582 | Train F1: 0.3359 | Valid Loss: 1.7285 | Valid Acc: 0.5458 | Valid F1: 0.4022\n",
            "[TextCNN] Epoch 4 | Train Loss: 1.7774 | Train Acc: 0.5206 | Train F1: 0.4002 | Valid Loss: 1.6180 | Valid Acc: 0.5731 | Valid F1: 0.4379\n",
            "[TextCNN] Epoch 5 | Train Loss: 1.6678 | Train Acc: 0.5443 | Train F1: 0.4231 | Valid Loss: 1.5335 | Valid Acc: 0.5959 | Valid F1: 0.4621\n",
            "    → accuracy:  0.5959\n",
            "    → precision: 0.4931\n",
            "    → recall:    0.4644\n",
            "    → f1:        0.4621\n",
            "    → model_size: 39089975 bytes\n",
            "\n",
            "▶開始實驗 (TextCNN/MLP): TextCNN__flat_section\n",
            "  [TextCNN] Step1: 開始訓練\n",
            "[TextCNN] Epoch 1 | Train Loss: 4.5078 | Train Acc: 0.0967 | Train F1: 0.0306 | Valid Loss: 3.7575 | Valid Acc: 0.2218 | Valid F1: 0.0775\n",
            "[TextCNN] Epoch 2 | Train Loss: 3.6458 | Train Acc: 0.2169 | Train F1: 0.0996 | Valid Loss: 3.1675 | Valid Acc: 0.3320 | Valid F1: 0.1623\n",
            "[TextCNN] Epoch 3 | Train Loss: 3.1096 | Train Acc: 0.3031 | Train F1: 0.1705 | Valid Loss: 2.7250 | Valid Acc: 0.3995 | Valid F1: 0.2310\n",
            "[TextCNN] Epoch 4 | Train Loss: 2.7381 | Train Acc: 0.3706 | Train F1: 0.2271 | Valid Loss: 2.5241 | Valid Acc: 0.4355 | Valid F1: 0.2627\n",
            "[TextCNN] Epoch 5 | Train Loss: 2.5690 | Train Acc: 0.3961 | Train F1: 0.2522 | Valid Loss: 2.3921 | Valid Acc: 0.4542 | Valid F1: 0.2823\n",
            "    → accuracy:  0.4542\n",
            "    → precision: 0.3219\n",
            "    → recall:    0.3070\n",
            "    → f1:        0.2823\n",
            "    → model_size: 38295037 bytes\n",
            "\n",
            "▶開始實驗 (TextCNN/MLP): TextCNN__hierarchical\n",
            "  [Hierarchical] Step1: 訓練 subject 分類器\n",
            "[TextCNN] Epoch 1 | Train Loss: 0.1217 | Train Acc: 0.9542 | Train F1: 0.9519 | Valid Loss: 0.0385 | Valid Acc: 0.9913 | Valid F1: 0.9909\n",
            "[TextCNN] Epoch 2 | Train Loss: 0.0383 | Train Acc: 0.9875 | Train F1: 0.9869 | Valid Loss: 0.0306 | Valid Acc: 0.9886 | Valid F1: 0.9880\n",
            "[TextCNN] Epoch 3 | Train Loss: 0.0288 | Train Acc: 0.9908 | Train F1: 0.9903 | Valid Loss: 0.0205 | Valid Acc: 0.9933 | Valid F1: 0.9930\n",
            "[TextCNN] Epoch 4 | Train Loss: 0.0194 | Train Acc: 0.9935 | Train F1: 0.9931 | Valid Loss: 0.0188 | Valid Acc: 0.9920 | Valid F1: 0.9916\n",
            "[TextCNN] Epoch 5 | Train Loss: 0.0151 | Train Acc: 0.9950 | Train F1: 0.9948 | Valid Loss: 0.0157 | Valid Acc: 0.9947 | Valid F1: 0.9944\n",
            "  [Hierarchical] Step2: 針對每個 subject 訓練 chapter 分類器\n",
            "[TextCNN] Epoch 1 | Train Loss: 2.5338 | Train Acc: 0.2765 | Train F1: 0.1990 | Valid Loss: 1.7773 | Valid Acc: 0.5241 | Valid F1: 0.4098\n",
            "[TextCNN] Epoch 2 | Train Loss: 1.7483 | Train Acc: 0.4988 | Train F1: 0.4176 | Valid Loss: 1.3551 | Valid Acc: 0.6291 | Valid F1: 0.5542\n",
            "[TextCNN] Epoch 3 | Train Loss: 1.4096 | Train Acc: 0.5862 | Train F1: 0.5229 | Valid Loss: 1.1580 | Valid Acc: 0.6772 | Valid F1: 0.6237\n",
            "[TextCNN] Epoch 4 | Train Loss: 1.2055 | Train Acc: 0.6463 | Train F1: 0.6016 | Valid Loss: 1.0784 | Valid Acc: 0.7013 | Valid F1: 0.6549\n",
            "[TextCNN] Epoch 5 | Train Loss: 1.1266 | Train Acc: 0.6720 | Train F1: 0.6246 | Valid Loss: 1.0205 | Valid Acc: 0.7221 | Valid F1: 0.6805\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1106b537accf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m     \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-1106b537accf>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"TextCNN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                         chap_m = train_textcnn(\n\u001b[0m\u001b[1;32m    754\u001b[0m                             \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                             \u001b[0mtrain_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub_train_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-1106b537accf>\u001b[0m in \u001b[0;36mtrain_textcnn\u001b[0;34m(vocab_size, train_texts, train_labels, valid_texts, valid_labels, tokenizer, num_labels, output_dir, device)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Dict, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType  # 只留 LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 固定亂數種子，確保結果可重現\n",
        "# =============================================================================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 資料載入與 Label Processing\n",
        "# =============================================================================\n",
        "def load_and_prepare(subjects: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    讀取多個 subject 的資料，合併後做 label encoding：\n",
        "      - label_subject\n",
        "      - label_chapter\n",
        "      - label_section\n",
        "    \"\"\"\n",
        "    def load_and_merge(subject: str) -> pd.DataFrame:\n",
        "        base_path = f\"{subject}_Database\"\n",
        "        qdf = pd.read_csv(f\"{base_path}/{subject}_question_bank.csv\")\n",
        "        cdf = pd.read_csv(f\"{base_path}/{subject}_chapter_list.csv\")\n",
        "        qdf.columns = qdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        cdf.columns = cdf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "        df = qdf.merge(cdf, on=\"section_name\", how=\"left\")\n",
        "        df[\"subject\"] = subject\n",
        "        return df\n",
        "\n",
        "    # 合併指定 subjects 的資料\n",
        "    df = pd.concat([load_and_merge(s) for s in subjects], ignore_index=True)\n",
        "    df = df[[\"subject\", \"chapter_name_x\", \"section_name\", \"ques_detl\"]].dropna().reset_index(drop=True)\n",
        "\n",
        "    # subject label\n",
        "    df[\"label_str\"] = df[\"subject\"]\n",
        "    label2id_subject = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_subject = {i: lab for lab, i in label2id_subject.items()}\n",
        "    df[\"label_subject\"] = df[\"label_str\"].map(label2id_subject)\n",
        "\n",
        "    # chapter label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"]\n",
        "    label2id_chapter = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_chapter = {i: lab for lab, i in label2id_chapter.items()}\n",
        "    df[\"label_chapter\"] = df[\"label_str\"].map(label2id_chapter)\n",
        "\n",
        "    # section label\n",
        "    df[\"label_str\"] = df[\"subject\"] + \"::\" + df[\"chapter_name_x\"] + \"::\" + df[\"section_name\"]\n",
        "    label2id_section = {lab: i for i, lab in enumerate(sorted(df[\"label_str\"].unique()))}\n",
        "    id2label_section = {i: lab for lab, i in label2id_section.items()}\n",
        "    df[\"label_section\"] = df[\"label_str\"].map(label2id_section)\n",
        "\n",
        "    # 只保留至少出現兩次的 section\n",
        "    vc = df[\"label_section\"].value_counts()\n",
        "    valid_secs = set(vc[vc >= 2].index)\n",
        "    df = df[df[\"label_section\"].isin(valid_secs)].reset_index(drop=True)\n",
        "\n",
        "    return df, (label2id_subject, id2label_subject), (label2id_chapter, id2label_chapter), (label2id_section, id2label_section)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 自訂 Dataset\n",
        "# =============================================================================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        labels: List[int],\n",
        "        tokenizer,\n",
        "        max_len: int = 128,\n",
        "        mode: str = \"flat_chapter\",  # 'flat_chapter', 'flat_section', 'hierarchical'\n",
        "        subject_labels: List[int] = None\n",
        "    ):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.mode = mode\n",
        "        self.subject_labels = subject_labels  # 只有 hierarchical 模式才需要\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        if self.mode == \"hierarchical\":\n",
        "            item[\"subject_labels\"] = torch.tensor(self.subject_labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# =============================================================================\n",
        "# 4. TextCNN Model 定義\n",
        "# =============================================================================\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_classes: int,\n",
        "        kernel_sizes: List[int] = [3,4,5],\n",
        "        num_filters: int = 100,\n",
        "        dropout_p: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (B, L)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.embedding(input_ids)        # (B, L, D)\n",
        "        x = x.permute(0, 2, 1)               # (B, D, L)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            c = conv(x)                      # (B, F, L - k + 1)\n",
        "            c = torch.relu(c)                # (B, F, L - k + 1)\n",
        "            c = torch.max_pool1d(c, kernel_size=c.size(2))  # (B, F, 1)\n",
        "            conv_outs.append(c.squeeze(2))   # (B, F)\n",
        "        cat = torch.cat(conv_outs, dim=1)    # (B, F * len(kernel_sizes))\n",
        "        drop = self.dropout(cat)             # (B, F * len(kernel_sizes))\n",
        "        logits = self.fc(drop)               # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 基礎 MLP Model 定義 (只用於 flat 模式)\n",
        "# =============================================================================\n",
        "class BasicMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        features: (B, input_dim)\n",
        "        return: logits (B, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.fc1(features)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 訓練與評估函式\n",
        "# =============================================================================\n",
        "def compute_metrics(preds_and_labels) -> Dict[str, float]:\n",
        "    logits, labels = preds_and_labels\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "def train_flat_transformer(\n",
        "    model_name: str,\n",
        "    num_labels: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    output_dir: str,\n",
        "    device: torch.device,\n",
        "    use_dora: bool = False\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 flat (Chapter or Section) 模式的小型 Transformer (BERT/ RoBERTa)，回傳 metrics。\n",
        "    如果 use_dora=True，會在模型上套 DoRA Adapter（LoraConfig）。\n",
        "    \"\"\"\n",
        "    # 1. 選擇 tokenizer & base_model\n",
        "    if \"roberta\" in model_name:\n",
        "        tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    else:\n",
        "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "        base_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    # 2. 如果要用 DoRA，包成 PEFT 模型\n",
        "    if use_dora:\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            inference_mode=False,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.05,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            use_dora=True,\n",
        "        )\n",
        "        model = get_peft_model(base_model, peft_config)\n",
        "    else:\n",
        "        model = base_model\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # 3. 構造 Dataset\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=128, mode=\"flat_chapter\")\n",
        "\n",
        "    # 4. TrainingArguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=8,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=3e-5 if not use_dora else 3e-4,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        logging_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    # 5. 初始化 Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # 6. 訓練 & 驗證\n",
        "    start = time.time()\n",
        "    trainer.train()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    # 7. 儲存 adapter（若有 DoRA）或整個模型\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # 8. 計算檔案大小\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    metrics[\"eval_accuracy\"],\n",
        "        \"precision\":   metrics[\"eval_precision\"],\n",
        "        \"recall\":      metrics[\"eval_recall\"],\n",
        "        \"f1\":          metrics[\"eval_f1\"],\n",
        "        \"train_time\":  elapsed,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "def train_textcnn(\n",
        "    vocab_size: int,\n",
        "    train_texts: List[str],\n",
        "    train_labels: List[int],\n",
        "    valid_texts: List[str],\n",
        "    valid_labels: List[int],\n",
        "    tokenizer,\n",
        "    num_labels: int,\n",
        "    output_dir: str,\n",
        "    device: torch.device\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    訓練 TextCNN (flat 模式)，回傳 metrics (accuracy, precision, recall, f1, train_time, model_size)。\n",
        "    \"\"\"\n",
        "    max_len = 128\n",
        "    batch_size = 32\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "    valid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, max_len=max_len, mode=\"flat_chapter\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    embed_dim = 300\n",
        "    model = TextCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_metrics = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "        train_acc = accuracy_score(all_labels, all_preds)\n",
        "        train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        total_vloss = 0.0\n",
        "        v_preds, v_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                logits = model(input_ids)\n",
        "                loss = criterion(logits, labels)\n",
        "                total_vloss += loss.item() * input_ids.size(0)\n",
        "                preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "                v_preds.extend(preds)\n",
        "                v_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        valid_loss = total_vloss / len(valid_loader.dataset)\n",
        "        valid_acc = accuracy_score(v_labels, v_preds)\n",
        "        valid_prec, valid_rec, valid_f1, _ = precision_recall_fscore_support(\n",
        "            v_labels, v_preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if valid_f1 > best_f1:\n",
        "            best_f1 = valid_f1\n",
        "            best_metrics = {\n",
        "                \"accuracy\":  valid_acc,\n",
        "                \"precision\": valid_prec,\n",
        "                \"recall\":    valid_rec,\n",
        "                \"f1\":        valid_f1\n",
        "            }\n",
        "            # 存模型前先確保 output_dir 存在\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/best_textcnn.pt\")\n",
        "\n",
        "        print(\n",
        "            f\"[TextCNN] Epoch {epoch} | \"\n",
        "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | \"\n",
        "            f\"Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f} | Valid F1: {valid_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for fname in files:\n",
        "            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\":    best_metrics[\"accuracy\"],\n",
        "        \"precision\":   best_metrics[\"precision\"],\n",
        "        \"recall\":      best_metrics[\"recall\"],\n",
        "        \"f1\":          best_metrics[\"f1\"],\n",
        "        \"train_time\":  elapsed,\n",
        "        \"model_size\":  total_size\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 7. Pipeline 主程序：整合上述所有 case\n",
        "# =============================================================================\n",
        "def main_pipeline():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. 載入資料與 Labels\n",
        "    df, subj_map, chap_map, sect_map = load_and_prepare([\"math\", \"science\"]) # 社會科改成合適的 subject\n",
        "    label2id_subject, id2label_subject = subj_map\n",
        "    label2id_chapter, id2label_chapter = chap_map\n",
        "    label2id_section, id2label_section = sect_map\n",
        "\n",
        "    # 2. 切 train/valid/test（90/10 → 再分81/9/10)\n",
        "    base = df[[\"ques_detl\", \"label_subject\", \"label_chapter\", \"label_section\", \"subject\"]].copy()\n",
        "\n",
        "    # 先切出 10% 做最終 test\n",
        "    rest, test = train_test_split(\n",
        "        base, test_size=0.1, stratify=base[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 再把 rest 切成 90/10 ≈ 81%/9% 做 train/valid\n",
        "    train_df, valid_df = train_test_split(\n",
        "        rest, test_size=0.1, stratify=rest[\"label_chapter\"], random_state=42\n",
        "    )\n",
        "\n",
        "    # 取出 text 和各種 label lists\n",
        "    train_texts = train_df[\"ques_detl\"].tolist()\n",
        "    valid_texts = valid_df[\"ques_detl\"].tolist()\n",
        "    test_texts  = test[\"ques_detl\"].tolist()\n",
        "\n",
        "    train_subj_labels = train_df[\"label_subject\"].tolist()\n",
        "    valid_subj_labels = valid_df[\"label_subject\"].tolist()\n",
        "    test_subj_labels  = test[\"label_subject\"].tolist()\n",
        "\n",
        "    train_chap_labels = train_df[\"label_chapter\"].tolist()\n",
        "    valid_chap_labels = valid_df[\"label_chapter\"].tolist()\n",
        "    test_chap_labels  = test[\"label_chapter\"].tolist()\n",
        "\n",
        "    train_sect_labels = train_df[\"label_section\"].tolist()\n",
        "    valid_sect_labels = valid_df[\"label_section\"].tolist()\n",
        "    test_sect_labels  = test[\"label_section\"].tolist()\n",
        "\n",
        "    # 3. 建立 tokenizer (給所有 TextCNN/Transformer 共用)\n",
        "    bert_tok    = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "    roberta_tok = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    # 4. 建立詞彙大小 (TextCNN 用)\n",
        "    vocab_size = bert_tok.vocab_size\n",
        "\n",
        "    # 5. 實驗配置\n",
        "    model_types = [\n",
        "        # 範例：(\"BERT\", \"bert-base-uncased\", False),\n",
        "        #        (\"RoBERTa\", \"roberta-base\", False),\n",
        "        #        (\"DoRA_BERT\", \"bert-base-uncased\", True),\n",
        "        #        (\"DoRA_RoBERTa\", \"roberta-base\", True),\n",
        "    ]\n",
        "    others = [\"MLP\"] #\"TextCNN\",\n",
        "\n",
        "    # 6. 三種分類策略\n",
        "    strategies = [\n",
        "        \"flat_chapter\",          # 直接分類 subject+chapter\n",
        "        \"flat_section\",          # 直接分類 subject+chapter+section\n",
        "        \"hierarchical\"           # 先分 subject，再分 chapter\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # (A) 先跑 Transformer 部分 (示例)\n",
        "    for model_name, hf_model_name, use_dora in model_types:\n",
        "        for strat in strategies:\n",
        "            exp_name = f\"{model_name}__{strat}\"\n",
        "            output_dir = f\"./outputs/{exp_name}\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            print(f\"\\n▶開始實驗 (Transformer): {exp_name}\")\n",
        "\n",
        "            if strat == \"flat_chapter\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_chapter),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_chap_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_chap_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"flat_section\":\n",
        "                metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_section),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_sect_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_sect_labels,\n",
        "                    output_dir=output_dir,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "                results.append((exp_name, metrics))\n",
        "\n",
        "            elif strat == \"hierarchical\":\n",
        "                # hierarchical: 先訓練 subject，再分 subj_id 針對 chapter\n",
        "                sub_output = os.path.join(output_dir, \"subj_model\")\n",
        "                chap_output = os.path.join(output_dir, \"chapter_model\")\n",
        "                os.makedirs(sub_output, exist_ok=True)\n",
        "                os.makedirs(chap_output, exist_ok=True)\n",
        "\n",
        "                print(\"  [Hierarchical] Step 1: 訓練 subject 模型\")\n",
        "                sub_metrics = train_flat_transformer(\n",
        "                    model_name=hf_model_name,\n",
        "                    num_labels=len(label2id_subject),\n",
        "                    train_texts=train_texts,\n",
        "                    train_labels=train_subj_labels,\n",
        "                    valid_texts=valid_texts,\n",
        "                    valid_labels=valid_subj_labels,\n",
        "                    output_dir=sub_output,\n",
        "                    device=device,\n",
        "                    use_dora=use_dora\n",
        "                )\n",
        "\n",
        "                chap_metrics = {}\n",
        "                print(\"  [Hierarchical] Step 2: 針對每個 subject 訓練 chapter 分類器\")\n",
        "                for subj_id, subj_name in id2label_subject.items():\n",
        "                    idx_train = [i for i, s in enumerate(train_subj_labels) if s == subj_id]\n",
        "                    idx_valid = [i for i, s in enumerate(valid_subj_labels) if s == subj_id]\n",
        "\n",
        "                    sub_train_texts = [train_texts[i] for i in idx_train]\n",
        "                    sub_train_chaps = [train_chap_labels[i] for i in idx_train]\n",
        "                    sub_valid_texts = [valid_texts[i] for i in idx_valid]\n",
        "                    sub_valid_chaps = [valid_chap_labels[i] for i in idx_valid]\n",
        "\n",
        "                    unique_chaps = sorted({train_chap_labels[i] for i in idx_train})\n",
        "                    if len(unique_chaps) < 2:\n",
        "                        continue\n",
        "\n",
        "                    sub_dir = os.path.join(chap_output, f\"subj_{subj_id}\")\n",
        "                    os.makedirs(sub_dir, exist_ok=True)\n",
        "\n",
        "                    print(f\"    [Subject {subj_id}] Training chapter classifier (num_labels={len(unique_chaps)})\")\n",
        "                    sub_chap_m = train_flat_transformer(\n",
        "                        model_name=hf_model_name,\n",
        "                        num_labels=len(unique_chaps),\n",
        "                        train_texts=sub_train_texts,\n",
        "                        train_labels=sub_train_chaps,\n",
        "                        valid_texts=sub_valid_texts,\n",
        "                        valid_labels=sub_valid_chaps,\n",
        "                        output_dir=sub_dir,\n",
        "                        device=device,\n",
        "                        use_dora=use_dora\n",
        "                    )\n",
        "                    chap_metrics[subj_id] = sub_chap_m\n",
        "\n",
        "                results.append((exp_name, {\"subject_metrics\": sub_metrics, \"chapter_metrics\": chap_metrics}))\n",
        "\n",
        "    # =============================================================================\n",
        "    # (B) TextCNN + MLP 只負責三種策略 (flat_chapter, flat_section, hierarchical)\n",
        "    # =============================================================================\n",
        "    for model_name in others:\n",
        "        for strat in strategies:\n",
        "            exp_name = f\"{model_name}__{strat}\"\n",
        "            output_dir = f\"./outputs/{exp_name}\"\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            print(f\"\\n▶開始實驗 (TextCNN/MLP): {exp_name}\")\n",
        "\n",
        "            # 如果是 hierarchical，先在這裡幫它建好 subdir\n",
        "            if strat == \"hierarchical\":\n",
        "                os.makedirs(os.path.join(output_dir, \"subj_model\"), exist_ok=True)\n",
        "                os.makedirs(os.path.join(output_dir, \"chapter_model\"), exist_ok=True)\n",
        "\n",
        "            if strat == \"flat_chapter\":\n",
        "                cur_train_labels = train_chap_labels\n",
        "                cur_valid_labels = valid_chap_labels\n",
        "                num_labels = len(label2id_chapter)\n",
        "\n",
        "            elif strat == \"flat_section\":\n",
        "                cur_train_labels = train_sect_labels\n",
        "                cur_valid_labels = valid_sect_labels\n",
        "                num_labels = len(label2id_section)\n",
        "\n",
        "            elif strat == \"hierarchical\":\n",
        "                cur_results = {\"subject\": {}, \"chapter\": {}}\n",
        "\n",
        "                # Step1: 訓練 subject 分類器\n",
        "                print(\"  [Hierarchical] Step1: 訓練 subject 分類器\")\n",
        "                # 先確保 subj_model 存在\n",
        "                subj_model_dir = os.path.join(output_dir, \"subj_model\")\n",
        "                os.makedirs(subj_model_dir, exist_ok=True)\n",
        "\n",
        "                if model_name == \"TextCNN\":\n",
        "                    # 用 chapter labels 當作 proxy 來當作 subject 分類（TextCNN 只支援 flat_chapter）\n",
        "                    sub_metrics = train_textcnn(\n",
        "                        vocab_size=vocab_size,\n",
        "                        train_texts=train_texts,\n",
        "                        train_labels=train_subj_labels,\n",
        "                        valid_texts=valid_texts,\n",
        "                        valid_labels=valid_subj_labels,\n",
        "                        tokenizer=bert_tok,\n",
        "                        num_labels=len(label2id_subject),\n",
        "                        output_dir=subj_model_dir,\n",
        "                        device=device\n",
        "                    )\n",
        "                else:  # MLP 版 subject\n",
        "                    enc_train = bert_tok(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "                    enc_valid = bert_tok(valid_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "                    bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                        \"bert-base-uncased\", output_hidden_states=True\n",
        "                    ).to(device)\n",
        "                    bert_model.eval()\n",
        "\n",
        "                    def encode_avg(inputs, batch_size=32):\n",
        "                        input_ids = inputs[\"input_ids\"]\n",
        "                        attention_mask = inputs[\"attention_mask\"]\n",
        "                        feats = []\n",
        "                        with torch.no_grad():\n",
        "                            for i in range(0, input_ids.size(0), batch_size):\n",
        "                                end_i = min(i + batch_size, input_ids.size(0))\n",
        "                                b_ids = input_ids[i:end_i].to(device)\n",
        "                                b_att = attention_mask[i:end_i].to(device)\n",
        "                                outs = bert_model.bert(b_ids, attention_mask=b_att).last_hidden_state\n",
        "                                mask = b_att.unsqueeze(-1).expand_as(outs).float()\n",
        "                                summed = torch.sum(outs * mask, dim=1)\n",
        "                                counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "                                avg = (summed / counts).cpu()\n",
        "                                feats.append(avg)\n",
        "                                del b_ids, b_att, outs, mask, summed, counts\n",
        "                                torch.cuda.empty_cache()\n",
        "                        return torch.cat(feats, dim=0)\n",
        "\n",
        "                    subj_train_feats = encode_avg(enc_train)\n",
        "                    subj_valid_feats = encode_avg(enc_valid)\n",
        "\n",
        "                    subj_model = BasicMLP(subj_train_feats.size(1), 256, len(label2id_subject)).to(device)\n",
        "                    subj_train_ds = TensorDataset(subj_train_feats, torch.tensor(train_subj_labels))\n",
        "                    subj_valid_ds = TensorDataset(subj_valid_feats, torch.tensor(valid_subj_labels))\n",
        "                    subj_train_loader = DataLoader(subj_train_ds, batch_size=64, shuffle=True)\n",
        "                    subj_valid_loader = DataLoader(subj_valid_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "                    opt = optim.Adam(subj_model.parameters(), lr=1e-4)\n",
        "                    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "                    best_f1_s = 0.0\n",
        "                    best_metrics_s = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "                    st = time.time()\n",
        "                    for epoch in range(1, 6):\n",
        "                        subj_model.train()\n",
        "                        train_preds, train_labels_acc = [], []\n",
        "                        run_loss = 0.0\n",
        "                        for xb, yb in subj_train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            opt.zero_grad()\n",
        "                            logits = subj_model(xb)\n",
        "                            loss = crit(logits, yb)\n",
        "                            loss.backward()\n",
        "                            opt.step()\n",
        "                            run_loss += loss.item() * xb.size(0)\n",
        "                            pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                            train_preds.extend(pr)\n",
        "                            train_labels_acc.extend(yb.cpu().numpy())\n",
        "\n",
        "                        train_acc_s = accuracy_score(train_labels_acc, train_preds)\n",
        "                        _, _, train_f1_s, _ = precision_recall_fscore_support(\n",
        "                            train_labels_acc, train_preds, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "\n",
        "                        subj_model.eval()\n",
        "                        val_preds_s, val_labels_s = [], []\n",
        "                        vloss_s = 0.0\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in subj_valid_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                logits = subj_model(xb)\n",
        "                                loss = crit(logits, yb)\n",
        "                                vloss_s += loss.item() * xb.size(0)\n",
        "                                pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                val_preds_s.extend(pr)\n",
        "                                val_labels_s.extend(yb.cpu().numpy())\n",
        "\n",
        "                        valid_acc_s = accuracy_score(val_labels_s, val_preds_s)\n",
        "                        _, _, valid_f1_s, _ = precision_recall_fscore_support(\n",
        "                            val_labels_s, val_preds_s, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "\n",
        "                        if valid_f1_s > best_f1_s:\n",
        "                            best_f1_s = valid_f1_s\n",
        "                            best_metrics_s = {\n",
        "                                \"accuracy\":  valid_acc_s,\n",
        "                                \"precision\": precision_score(val_labels_s, val_preds_s, average=\"macro\", zero_division=0),\n",
        "                                \"recall\":    recall_score(val_labels_s, val_preds_s, average=\"macro\", zero_division=0),\n",
        "                                \"f1\":        valid_f1_s\n",
        "                            }\n",
        "                            # 確保子目錄存在\n",
        "                            save_dir_s = os.path.join(output_dir, \"subj_model\")\n",
        "                            os.makedirs(save_dir_s, exist_ok=True)\n",
        "                            torch.save(subj_model.state_dict(), os.path.join(save_dir_s, \"best_subj_mlp.pt\"))\n",
        "\n",
        "                        print(\n",
        "                            f\"    [Subject MLP Epoch {epoch}] \"\n",
        "                            f\"Train Acc: {train_acc_s:.4f} | Train F1: {train_f1_s:.4f} | \"\n",
        "                            f\"Valid Acc: {valid_acc_s:.4f} | Valid F1: {valid_f1_s:.4f}\"\n",
        "                        )\n",
        "\n",
        "                    elapsed_s = time.time() - st\n",
        "                    total_sz_s = 0\n",
        "                    save_dir_s = os.path.join(output_dir, \"subj_model\")\n",
        "                    for root, _, files in os.walk(save_dir_s):\n",
        "                        for fname in files:\n",
        "                            total_sz_s += os.path.getsize(os.path.join(root, fname))\n",
        "                    sub_metrics = {\n",
        "                        \"accuracy\":    best_metrics_s[\"accuracy\"],\n",
        "                        \"precision\":   best_metrics_s[\"precision\"],\n",
        "                        \"recall\":      best_metrics_s[\"recall\"],\n",
        "                        \"f1\":          best_metrics_s[\"f1\"],\n",
        "                        \"train_time\":  elapsed_s,\n",
        "                        \"model_size\":  total_sz_s\n",
        "                    }\n",
        "\n",
        "                cur_results[\"subject\"][model_name] = sub_metrics  # TextCNN 或 MLP 的 subject 部分\n",
        "\n",
        "                # Step2: 針對每個 subject 訓練 chapter 分類器\n",
        "                print(\"  [Hierarchical] Step2: 針對每個 subject 訓練 chapter 分類器\")\n",
        "                for subj_id, subj_name in id2label_subject.items():\n",
        "                    idx_train = [i for i, s in enumerate(train_subj_labels) if s == subj_id]\n",
        "                    idx_valid = [i for i, s in enumerate(valid_subj_labels) if s == subj_id]\n",
        "\n",
        "                    sub_train_texts = [train_texts[i] for i in idx_train]\n",
        "                    sub_train_chaps = [train_chap_labels[i] for i in idx_train]\n",
        "                    sub_valid_texts = [valid_texts[i] for i in idx_valid]\n",
        "                    sub_valid_chaps = [valid_chap_labels[i] for i in idx_valid]\n",
        "\n",
        "                    unique_chaps = sorted({train_chap_labels[i] for i in idx_train})\n",
        "                    if len(unique_chaps) < 2:\n",
        "                        continue\n",
        "\n",
        "                    chap_dir = os.path.join(output_dir, \"chapter_model\", f\"subj_{subj_id}\")\n",
        "                    os.makedirs(chap_dir, exist_ok=True)\n",
        "\n",
        "                    if model_name == \"TextCNN\":\n",
        "                        chap_m = train_textcnn(\n",
        "                            vocab_size=vocab_size,\n",
        "                            train_texts=sub_train_texts,\n",
        "                            train_labels=sub_train_chaps,\n",
        "                            valid_texts=sub_valid_texts,\n",
        "                            valid_labels=sub_valid_chaps,\n",
        "                            tokenizer=bert_tok,\n",
        "                            num_labels=len(unique_chaps),\n",
        "                            output_dir=chap_dir,\n",
        "                            device=device\n",
        "                        )\n",
        "                    else:  # MLP\n",
        "                        enc_tr = bert_tok(sub_train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "                        enc_vd = bert_tok(sub_valid_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "                        # 載入 BERT 作特徵擷取\n",
        "                        bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                            \"bert-base-uncased\", output_hidden_states=True\n",
        "                        ).to(device)\n",
        "                        bert_model.eval()\n",
        "\n",
        "                        def encode_avg(inputs, batch_size=32):\n",
        "                            input_ids = inputs[\"input_ids\"]\n",
        "                            attention_mask = inputs[\"attention_mask\"]\n",
        "                            feats = []\n",
        "                            with torch.no_grad():\n",
        "                                for i in range(0, input_ids.size(0), batch_size):\n",
        "                                    end_i = min(i + batch_size, input_ids.size(0))\n",
        "                                    b_ids = input_ids[i:end_i].to(device)\n",
        "                                    b_att = attention_mask[i:end_i].to(device)\n",
        "                                    outs = bert_model.bert(b_ids, attention_mask=b_att).last_hidden_state\n",
        "                                    mask = b_att.unsqueeze(-1).expand_as(outs).float()\n",
        "                                    summed = torch.sum(outs * mask, dim=1)\n",
        "                                    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "                                    avg = (summed / counts).cpu()\n",
        "                                    feats.append(avg)\n",
        "                                    del b_ids, b_att, outs, mask, summed, counts\n",
        "                                    torch.cuda.empty_cache()\n",
        "                            return torch.cat(feats, dim=0)\n",
        "\n",
        "                        tr_feats = encode_avg(enc_tr)\n",
        "                        vd_feats = encode_avg(enc_vd)\n",
        "\n",
        "                        mlp_model = BasicMLP(tr_feats.size(1), 256, len(unique_chaps)).to(device)\n",
        "                        tr_ds = TensorDataset(tr_feats, torch.tensor(sub_train_chaps))\n",
        "                        vd_ds = TensorDataset(vd_feats, torch.tensor(sub_valid_chaps))\n",
        "                        tr_loader = DataLoader(tr_ds, batch_size=64, shuffle=True)\n",
        "                        vd_loader = DataLoader(vd_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "                        opt_mc = optim.Adam(mlp_model.parameters(), lr=1e-4)\n",
        "                        crit_mc = nn.CrossEntropyLoss()\n",
        "\n",
        "                        best_f1_c = 0.0\n",
        "                        best_metrics_c = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "                        st_c = time.time()\n",
        "                        for epoch in range(1, 6):\n",
        "                            mlp_model.train()\n",
        "                            preds_tr, labels_tr = [], []\n",
        "                            runloss = 0.0\n",
        "                            for xb, yb in tr_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                opt_mc.zero_grad()\n",
        "                                logits = mlp_model(xb)\n",
        "                                loss = crit_mc(logits, yb)\n",
        "                                loss.backward()\n",
        "                                opt_mc.step()\n",
        "                                runloss += loss.item() * xb.size(0)\n",
        "                                pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                preds_tr.extend(pr)\n",
        "                                labels_tr.extend(yb.cpu().numpy())\n",
        "                            acc_tr_c = accuracy_score(labels_tr, preds_tr)\n",
        "                            _, _, f1_tr_c, _ = precision_recall_fscore_support(\n",
        "                                labels_tr, preds_tr, average=\"macro\", zero_division=0\n",
        "                            )\n",
        "\n",
        "                            mlp_model.eval()\n",
        "                            preds_vd, labels_vd = [], []\n",
        "                            vl = 0.0\n",
        "                            with torch.no_grad():\n",
        "                                for xb, yb in vd_loader:\n",
        "                                    xb, yb = xb.to(device), yb.to(device)\n",
        "                                    logits = mlp_model(xb)\n",
        "                                    loss = crit_mc(logits, yb)\n",
        "                                    vl += loss.item() * xb.size(0)\n",
        "                                    pr = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                    preds_vd.extend(pr)\n",
        "                                    labels_vd.extend(yb.cpu().numpy())\n",
        "                            acc_vd_c = accuracy_score(labels_vd, preds_vd)\n",
        "                            _, _, f1_vd_c, _ = precision_recall_fscore_support(\n",
        "                                labels_vd, preds_vd, average=\"macro\", zero_division=0\n",
        "                            )\n",
        "                            if f1_vd_c > best_f1_c:\n",
        "                                best_f1_c = f1_vd_c\n",
        "                                best_metrics_c = {\n",
        "                                    \"accuracy\":  acc_vd_c,\n",
        "                                    \"precision\": precision_score(labels_vd, preds_vd, average=\"macro\", zero_division=0),\n",
        "                                    \"recall\":    recall_score(labels_vd, preds_vd, average=\"macro\", zero_division=0),\n",
        "                                    \"f1\":        f1_vd_c\n",
        "                                }\n",
        "                                # 確保 chap_dir 存在\n",
        "                                os.makedirs(chap_dir, exist_ok=True)\n",
        "                                torch.save(mlp_model.state_dict(), os.path.join(chap_dir, \"best_chap_mlp.pt\"))\n",
        "\n",
        "                            print(\n",
        "                                f\"    [Chapter MLP subj_{subj_id} Epoch {epoch}] \"\n",
        "                                f\"Train Acc: {acc_tr_c:.4f} | Train F1: {f1_tr_c:.4f} | \"\n",
        "                                f\"Valid Acc: {acc_vd_c:.4f} | Valid F1: {f1_vd_c:.4f}\"\n",
        "                            )\n",
        "\n",
        "                        elapsed_c = time.time() - st_c\n",
        "                        total_sz_c = 0\n",
        "                        for root, _, files in os.walk(chap_dir):\n",
        "                            for fname in files:\n",
        "                                total_sz_c += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "                        chap_m = {\n",
        "                            \"accuracy\":    best_metrics_c[\"accuracy\"],\n",
        "                            \"precision\":   best_metrics_c[\"precision\"],\n",
        "                            \"recall\":      best_metrics_c[\"recall\"],\n",
        "                            \"f1\":          best_metrics_c[\"f1\"],\n",
        "                            \"train_time\":  elapsed_c,\n",
        "                            \"model_size\":  total_sz_c\n",
        "                        }\n",
        "\n",
        "                    cur_results[\"chapter\"][subj_id] = chap_m\n",
        "\n",
        "                results.append((exp_name, cur_results))\n",
        "                continue  # 跳過後面的 flat 處理\n",
        "\n",
        "            # 以下為 flat_chapter 或 flat_section 的 TextCNN/MLP 處理\n",
        "            if strat in [\"flat_chapter\", \"flat_section\"]:\n",
        "                if strat == \"flat_chapter\":\n",
        "                    cur_train_labels = train_chap_labels\n",
        "                    cur_valid_labels = valid_chap_labels\n",
        "                    num_labels = len(label2id_chapter)\n",
        "                else:  # flat_section\n",
        "                    cur_train_labels = train_sect_labels\n",
        "                    cur_valid_labels = valid_sect_labels\n",
        "                    num_labels = len(label2id_section)\n",
        "\n",
        "                if model_name == \"TextCNN\":\n",
        "                    print(\"  [TextCNN] Step1: 開始訓練\")\n",
        "                    start = time.time()\n",
        "\n",
        "                    metrics = train_textcnn(\n",
        "                        vocab_size=vocab_size,\n",
        "                        train_texts=train_texts,\n",
        "                        train_labels=cur_train_labels,\n",
        "                        valid_texts=valid_texts,\n",
        "                        valid_labels=cur_valid_labels,\n",
        "                        tokenizer=bert_tok,\n",
        "                        num_labels=num_labels,\n",
        "                        output_dir=output_dir,\n",
        "                        device=device\n",
        "                    )\n",
        "                    elapsed = time.time() - start\n",
        "                    metrics[\"train_time\"] = elapsed\n",
        "\n",
        "                    print(f\"    → accuracy:  {metrics['accuracy']:.4f}\")\n",
        "                    print(f\"    → precision: {metrics['precision']:.4f}\")\n",
        "                    print(f\"    → recall:    {metrics['recall']:.4f}\")\n",
        "                    print(f\"    → f1:        {metrics['f1']:.4f}\")\n",
        "                    print(f\"    → model_size: {metrics['model_size']} bytes\")\n",
        "\n",
        "                    results.append((exp_name, metrics))\n",
        "\n",
        "                else:  # MLP\n",
        "                    print(\"  [MLP] Step1: 用 tokenizer 編碼到 CPU\")\n",
        "                    train_enc = bert_tok(\n",
        "                        train_texts,\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    valid_enc = bert_tok(\n",
        "                        valid_texts,\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    print(f\"    → train_enc.shape: {train_enc['input_ids'].shape}\")\n",
        "                    print(f\"    → valid_enc.shape: {train_enc['input_ids'].shape}\")\n",
        "\n",
        "                    print(\"  [MLP] Step2: 載入 BERT 做特徵擷取\")\n",
        "                    bert_model = BertForSequenceClassification.from_pretrained(\n",
        "                        \"bert-base-uncased\", output_hidden_states=True\n",
        "                    ).to(device)\n",
        "                    bert_model.eval()\n",
        "\n",
        "                    def encode_avg_dataset(inputs, batch_size: int = 32):\n",
        "                        input_ids = inputs[\"input_ids\"]\n",
        "                        attention_mask = inputs[\"attention_mask\"]\n",
        "                        features_list = []\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            for i in range(0, input_ids.size(0), batch_size):\n",
        "                                end = min(i + batch_size, input_ids.size(0))\n",
        "                                b_ids = input_ids[i:end].to(device)\n",
        "                                b_att = attention_mask[i:end].to(device)\n",
        "                                outs = bert_model.bert(b_ids, attention_mask=b_att).last_hidden_state\n",
        "                                mask = b_att.unsqueeze(-1).expand_as(outs).float()\n",
        "                                summed = torch.sum(outs * mask, dim=1)\n",
        "                                counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "                                avg = (summed / counts).cpu()\n",
        "                                features_list.append(avg)\n",
        "                                del b_ids, b_att, outs, mask, summed, counts\n",
        "                                torch.cuda.empty_cache()\n",
        "                        return torch.cat(features_list, dim=0)\n",
        "\n",
        "                    print(\"  [MLP] Step3: 計算 train_feats\")\n",
        "                    train_feats = encode_avg_dataset(train_enc, batch_size=32)\n",
        "                    print(f\"    → train_feats.shape = {train_feats.shape}\")\n",
        "\n",
        "                    print(\"  [MLP] Step4: 計算 valid_feats\")\n",
        "                    valid_feats = encode_avg_dataset(valid_enc, batch_size=32)\n",
        "                    print(f\"    → valid_feats.shape = {valid_feats.shape}\")\n",
        "\n",
        "                    print(\"  [MLP] Step5: 建立 MLP 並訓練\")\n",
        "                    input_dim = train_feats.size(1)\n",
        "                    hidden_dim = 256\n",
        "                    model = BasicMLP(input_dim, hidden_dim, num_labels).to(device)\n",
        "\n",
        "                    train_labels_tensor = torch.tensor(cur_train_labels, dtype=torch.long)\n",
        "                    valid_labels_tensor = torch.tensor(cur_valid_labels, dtype=torch.long)\n",
        "\n",
        "                    train_ds = TensorDataset(train_feats, train_labels_tensor)\n",
        "                    valid_ds = TensorDataset(valid_feats, valid_labels_tensor)\n",
        "                    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "                    valid_loader = DataLoader(valid_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    start = time.time()\n",
        "                    best_f1 = 0.0\n",
        "                    best_metrics = {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "                    for epoch in range(1, 6):\n",
        "                        model.train()\n",
        "                        epoch_loss = 0.0\n",
        "                        train_preds, train_labels_acc = [], []\n",
        "\n",
        "                        for feats_batch, labels_batch in train_loader:\n",
        "                            feats_batch = feats_batch.to(device)\n",
        "                            labels_batch = labels_batch.to(device)\n",
        "\n",
        "                            optimizer.zero_grad()\n",
        "                            logits = model(feats_batch)\n",
        "                            loss = criterion(logits, labels_batch)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            epoch_loss += loss.item() * feats_batch.size(0)\n",
        "                            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                            train_preds.extend(preds)\n",
        "                            train_labels_acc.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "                        train_acc = accuracy_score(train_labels_acc, train_preds)\n",
        "                        train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
        "                            train_labels_acc, train_preds, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "                        avg_train_loss = epoch_loss / len(train_loader.dataset)\n",
        "\n",
        "                        model.eval()\n",
        "                        valid_preds, valid_labels_acc = [], []\n",
        "                        vloss = 0.0\n",
        "                        with torch.no_grad():\n",
        "                            for feats_batch, labels_batch in valid_loader:\n",
        "                                feats_batch = feats_batch.to(device)\n",
        "                                labels_batch = labels_batch.to(device)\n",
        "                                logits = model(feats_batch)\n",
        "                                loss = criterion(logits, labels_batch)\n",
        "                                vloss += loss.item() * feats_batch.size(0)\n",
        "                                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                                valid_preds.extend(preds)\n",
        "                                valid_labels_acc.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "                        valid_acc = accuracy_score(valid_labels_acc, valid_preds)\n",
        "                        valid_prec, valid_rec, valid_f1, _ = precision_recall_fscore_support(\n",
        "                            valid_labels_acc, valid_preds, average=\"macro\", zero_division=0\n",
        "                        )\n",
        "\n",
        "                        if valid_f1 > best_f1:\n",
        "                            best_f1 = valid_f1\n",
        "                            best_metrics = {\n",
        "                                \"accuracy\":  valid_acc,\n",
        "                                \"precision\": valid_prec,\n",
        "                                \"recall\":    valid_rec,\n",
        "                                \"f1\":        valid_f1\n",
        "                            }\n",
        "                            # 確保 output_dir 存在\n",
        "                            os.makedirs(output_dir, exist_ok=True)\n",
        "                            torch.save(model.state_dict(), f\"{output_dir}/best_mlp.pt\")\n",
        "\n",
        "                        print(\n",
        "                            f\"[MLP Epoch {epoch}] \"\n",
        "                            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "                            f\"Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | \"\n",
        "                            f\"Valid Acc: {valid_acc:.4f} | Valid F1: {valid_f1:.4f}\"\n",
        "                        )\n",
        "\n",
        "                    elapsed = time.time() - start\n",
        "                    total_size = 0\n",
        "                    for root, _, files in os.walk(output_dir):\n",
        "                        for fname in files:\n",
        "                            total_size += os.path.getsize(os.path.join(root, fname))\n",
        "\n",
        "                    best_metrics[\"train_time\"] = elapsed\n",
        "                    best_metrics[\"model_size\"] = total_size\n",
        "                    print(f\"    → accuracy:  {best_metrics['accuracy']:.4f}\")\n",
        "                    print(f\"    → precision: {best_metrics['precision']:.4f}\")\n",
        "                    print(f\"    → recall:    {best_metrics['recall']:.4f}\")\n",
        "                    print(f\"    → f1:        {best_metrics['f1']:.4f}\")\n",
        "                    print(f\"    → model_size: {best_metrics['model_size']} bytes\")\n",
        "\n",
        "                    results.append((exp_name, best_metrics))\n",
        "\n",
        "    # =============================================================================\n",
        "    # 8. 把所有實驗結果匯出\n",
        "    # =============================================================================\n",
        "    out_df = pd.DataFrame([{\"experiment\": name, **metrics} for name, metrics in results])\n",
        "    out_df.to_csv(\"experiment_results.csv\", index=False)\n",
        "    print(\"\\n所有實驗完成，結果已存到 experiment_results.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C3YP7hKyUGpY",
        "outputId": "94ae5217-6113-49f8-d75f-522fa1281a98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶開始實驗 (TextCNN/MLP): MLP__flat_chapter\n",
            "  [MLP] Step1: 用 tokenizer 編碼到 CPU\n",
            "    → train_enc.shape: torch.Size([13469, 512])\n",
            "    → valid_enc.shape: torch.Size([13469, 512])\n",
            "  [MLP] Step2: 載入 BERT 做特徵擷取\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MLP] Step3: 計算 train_feats\n",
            "    → train_feats.shape = torch.Size([13469, 768])\n",
            "  [MLP] Step4: 計算 valid_feats\n",
            "    → valid_feats.shape = torch.Size([1497, 768])\n",
            "  [MLP] Step5: 建立 MLP 並訓練\n",
            "[MLP Epoch 1] Train Loss: 3.8882 | Train Acc: 0.0771 | Train F1: 0.0159 | Valid Acc: 0.1149 | Valid F1: 0.0187\n",
            "[MLP Epoch 2] Train Loss: 3.5708 | Train Acc: 0.1150 | Train F1: 0.0306 | Valid Acc: 0.1630 | Valid F1: 0.0372\n",
            "[MLP Epoch 3] Train Loss: 3.3779 | Train Acc: 0.1402 | Train F1: 0.0440 | Valid Acc: 0.1790 | Valid F1: 0.0433\n",
            "[MLP Epoch 4] Train Loss: 3.2432 | Train Acc: 0.1634 | Train F1: 0.0567 | Valid Acc: 0.1884 | Valid F1: 0.0522\n",
            "[MLP Epoch 5] Train Loss: 3.1460 | Train Acc: 0.1728 | Train F1: 0.0628 | Valid Acc: 0.2098 | Valid F1: 0.0642\n",
            "    → accuracy:  0.2098\n",
            "    → precision: 0.0738\n",
            "    → recall:    0.1001\n",
            "    → f1:        0.0642\n",
            "    → model_size: 853272 bytes\n",
            "\n",
            "▶開始實驗 (TextCNN/MLP): MLP__flat_section\n",
            "  [MLP] Step1: 用 tokenizer 編碼到 CPU\n",
            "    → train_enc.shape: torch.Size([13469, 512])\n",
            "    → valid_enc.shape: torch.Size([13469, 512])\n",
            "  [MLP] Step2: 載入 BERT 做特徵擷取\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MLP] Step3: 計算 train_feats\n",
            "    → train_feats.shape = torch.Size([13469, 768])\n",
            "  [MLP] Step4: 計算 valid_feats\n",
            "    → valid_feats.shape = torch.Size([1497, 768])\n",
            "  [MLP] Step5: 建立 MLP 並訓練\n",
            "[MLP Epoch 1] Train Loss: 5.0378 | Train Acc: 0.0279 | Train F1: 0.0031 | Valid Acc: 0.0508 | Valid F1: 0.0028\n",
            "[MLP Epoch 2] Train Loss: 4.7460 | Train Acc: 0.0511 | Train F1: 0.0065 | Valid Acc: 0.0735 | Valid F1: 0.0052\n",
            "[MLP Epoch 3] Train Loss: 4.5397 | Train Acc: 0.0679 | Train F1: 0.0104 | Valid Acc: 0.0828 | Valid F1: 0.0103\n",
            "[MLP Epoch 4] Train Loss: 4.4081 | Train Acc: 0.0760 | Train F1: 0.0137 | Valid Acc: 0.0908 | Valid F1: 0.0119\n",
            "[MLP Epoch 5] Train Loss: 4.2942 | Train Acc: 0.0852 | Train F1: 0.0171 | Valid Acc: 0.1129 | Valid F1: 0.0214\n",
            "    → accuracy:  0.1129\n",
            "    → precision: 0.0185\n",
            "    → recall:    0.0419\n",
            "    → f1:        0.0214\n",
            "    → model_size: 980760 bytes\n",
            "\n",
            "▶開始實驗 (TextCNN/MLP): MLP__hierarchical\n",
            "  [Hierarchical] Step1: 訓練 subject 分類器\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'precision_score' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-88c5183bad14>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m     \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-88c5183bad14>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    700\u001b[0m                             best_metrics_s = {\n\u001b[1;32m    701\u001b[0m                                 \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mvalid_acc_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                                 \u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m                                 \u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                                 \u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0mvalid_f1_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'precision_score' is not defined"
          ]
        }
      ]
    }
  ]
}