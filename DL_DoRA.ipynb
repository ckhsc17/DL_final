{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1PP8GhTDoxg"
      },
      "source": [
        "## Packages import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2SLtTNCSOgO8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRtlybIfA8bQ",
        "outputId": "ce60812f-0e5b-4740-8b53-9a487dc7c2d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytesseract in ./.venv/lib/python3.12/site-packages (0.3.13)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.52.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pytesseract pandas scikit-learn transformers json\n",
        "%pip install torch\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Optional OCR tools\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "# or use EasyOCR:\n",
        "# import easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVj1O-69EN49",
        "outputId": "4a5bf68e-07d6-4779-edba-cd515ea2c0f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用 Apple M1 GPU (MPS)\n"
          ]
        }
      ],
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "\n",
        "# Step 1: 自動選擇 device\n",
        "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"使用 Apple M1 GPU (MPS)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"使用 CPU（找不到 MPS）\")\n",
        "# # Check GPU\n",
        "# if torch.cuda.is_available():\n",
        "#     device = torch.device('mps')\n",
        "#     print(f'使用 GPU：{torch.cuda.get_device_name(0)}')\n",
        "# else:\n",
        "#     device = torch.device('cpu')\n",
        "#     print('使用 CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6kztd1CKCXI"
      },
      "source": [
        "## Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "SZcnhYfIESMv",
        "outputId": "a1753eb7-1368-40c1-b911-863bfa1e6aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question Bank Columns: ['subject', 'ques_no', 'chapter_name', 'section_name', 'ques_detl']\n",
            "Chapter List Columns: ['year_grade', 'book', 'chapter_num', 'chapter_name', 'section_num', 'section_name']\n",
            "Question Bank Columns: ['subject', 'ques_no', 'chapter_name', 'section_name', 'ques_detl']\n",
            "Chapter List Columns: ['year_grade', 'book', 'chapter_num', 'chapter_name', 'section_num', 'section_name']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>ques_no</th>\n",
              "      <th>chapter_name_x</th>\n",
              "      <th>section_name</th>\n",
              "      <th>ques_detl</th>\n",
              "      <th>year_grade</th>\n",
              "      <th>book</th>\n",
              "      <th>chapter_num</th>\n",
              "      <th>chapter_name_y</th>\n",
              "      <th>section_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>math</td>\n",
              "      <td>2MA0810001</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>以中午12時為基準，下午3時記作＋3，那麼上午10時可以記作多少？(A)＋10(B)－10(...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>math</td>\n",
              "      <td>2MA0810002</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>若數學科成績以30分為基準，得40分記作＋10，那麼考25分可記作多少？\u000b(A)－25　(B...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>math</td>\n",
              "      <td>2MA0810003</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>如果以正午12時為基準，當日下午4時用＋4表示，則當日上午8時應記為多少？\u000b(A)－8　(B...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>math</td>\n",
              "      <td>2MA0810004</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>如果以中午12時為基準，當日下午2時記為＋8，則當日上午6時應記為多少？(A)－24(B)－...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>math</td>\n",
              "      <td>2MA0810005</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>如果＋40°表示北緯40°，則南緯30°該如何表示？\u000b(A)30°　(B)－30°  (C)...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subject     ques_no chapter_name_x section_name  \\\n",
              "0    math  2MA0810001          整數的運算        負數與數線   \n",
              "1    math  2MA0810002          整數的運算        負數與數線   \n",
              "2    math  2MA0810003          整數的運算        負數與數線   \n",
              "3    math  2MA0810004          整數的運算        負數與數線   \n",
              "4    math  2MA0810005          整數的運算        負數與數線   \n",
              "\n",
              "                                           ques_detl  year_grade  book  \\\n",
              "0  以中午12時為基準，下午3時記作＋3，那麼上午10時可以記作多少？(A)＋10(B)－10(...         8.0   1.0   \n",
              "1  若數學科成績以30分為基準，得40分記作＋10，那麼考25分可記作多少？\n",
              "(A)－25　(B...         8.0   1.0   \n",
              "2  如果以正午12時為基準，當日下午4時用＋4表示，則當日上午8時應記為多少？\n",
              "(A)－8　(B...         8.0   1.0   \n",
              "3  如果以中午12時為基準，當日下午2時記為＋8，則當日上午6時應記為多少？(A)－24(B)－...         8.0   1.0   \n",
              "4  如果＋40°表示北緯40°，則南緯30°該如何表示？\n",
              "(A)30°　(B)－30°  (C)...         8.0   1.0   \n",
              "\n",
              "   chapter_num chapter_name_y  section_num  \n",
              "0          1.0          整數的運算          1.0  \n",
              "1          1.0          整數的運算          1.0  \n",
              "2          1.0          整數的運算          1.0  \n",
              "3          1.0          整數的運算          1.0  \n",
              "4          1.0          整數的運算          1.0  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_and_merge(subject: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load question bank and chapter list CSVs for a given subject from Database/{subject}_Database and merge on chapter ID.\n",
        "\n",
        "    Args:\n",
        "        subject: 'math' or 'social'\n",
        "    Returns:\n",
        "        DataFrame with columns: question_text, chapter_name, subject\n",
        "    \"\"\"\n",
        "    base_path = f\"Database/{subject}_Database\"\n",
        "    qbank_path = f\"{base_path}/{subject}_question_bank.csv\"\n",
        "    chap_path = f\"{base_path}/{subject}_chapter_list.csv\"\n",
        "    \n",
        "    qdf = pd.read_csv(qbank_path)\n",
        "    cdf = pd.read_csv(chap_path)\n",
        "    qdf.columns = qdf.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    cdf.columns = cdf.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "    print(\"Question Bank Columns:\", qdf.columns.tolist())\n",
        "    print(\"Chapter List Columns:\", cdf.columns.tolist())\n",
        "\n",
        "\n",
        "    df = qdf.merge(cdf, on='section_name', how='left')\n",
        "    df['subject'] = subject\n",
        "    return df\n",
        "\n",
        "# Load and combine data from correct paths\n",
        "df = pd.concat([load_and_merge('math'), load_and_merge('social')], ignore_index=True)\n",
        "\n",
        "df.count()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "C2K-oAKnKlbY",
        "outputId": "5f522948-22c6-44df-f8c7-e30c60ff1d96"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>chapter_name_x</th>\n",
              "      <th>section_name</th>\n",
              "      <th>ques_detl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>math</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>以中午12時為基準，下午3時記作＋3，那麼上午10時可以記作多少？(A)＋10(B)－10(...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>math</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>若數學科成績以30分為基準，得40分記作＋10，那麼考25分可記作多少？\u000b(A)－25　(B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>math</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>如果以正午12時為基準，當日下午4時用＋4表示，則當日上午8時應記為多少？\u000b(A)－8　(B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>math</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>如果以中午12時為基準，當日下午2時記為＋8，則當日上午6時應記為多少？(A)－24(B)－...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>math</td>\n",
              "      <td>整數的運算</td>\n",
              "      <td>負數與數線</td>\n",
              "      <td>如果＋40°表示北緯40°，則南緯30°該如何表示？\u000b(A)30°　(B)－30°  (C)...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subject chapter_name_x section_name  \\\n",
              "0    math          整數的運算        負數與數線   \n",
              "1    math          整數的運算        負數與數線   \n",
              "2    math          整數的運算        負數與數線   \n",
              "3    math          整數的運算        負數與數線   \n",
              "4    math          整數的運算        負數與數線   \n",
              "\n",
              "                                           ques_detl  \n",
              "0  以中午12時為基準，下午3時記作＋3，那麼上午10時可以記作多少？(A)＋10(B)－10(...  \n",
              "1  若數學科成績以30分為基準，得40分記作＋10，那麼考25分可記作多少？\n",
              "(A)－25　(B...  \n",
              "2  如果以正午12時為基準，當日下午4時用＋4表示，則當日上午8時應記為多少？\n",
              "(A)－8　(B...  \n",
              "3  如果以中午12時為基準，當日下午2時記為＋8，則當日上午6時應記為多少？(A)－24(B)－...  \n",
              "4  如果＋40°表示北緯40°，則南緯30°該如何表示？\n",
              "(A)30°　(B)－30°  (C)...  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[['subject','chapter_name_x','section_name','ques_detl']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "ZMGU1kj-JjuO",
        "outputId": "7c6712f6-2d21-4664-a003-a55d72f32182"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "subject           33839\n",
              "chapter_name_x    33839\n",
              "section_name      33839\n",
              "ques_detl         33839\n",
              "dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU818S_rJ9-Y",
        "outputId": "f1bfdda4-0286-47e4-eec6-e3493defcc64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33839"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kpzeLe4RKJzZ"
      },
      "outputs": [],
      "source": [
        "# 2. Label Encoding\n",
        "# Combine subject and chapter_name as classification label\n",
        "\n",
        "df['label_str'] = df['subject'] + '::' + df['chapter_name_x']\n",
        "label2id = {lab: i for i, lab in enumerate(sorted(df['label_str'].unique()))}\n",
        "id2label = {i: lab for lab, i in label2id.items()}\n",
        "df['label'] = df['label_str'].map(label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_y24qatKGl8",
        "outputId": "a6fe1e40-f33d-421f-80f3-6f3e4043c336"
      },
      "outputs": [],
      "source": [
        "# 3. Train/Test Split\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, stratify=df['label'], random_state=42)\n",
        "\n",
        "# 4. Tokenization and Dataset Definition\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class QDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        inputs = self.tokenizer(text,\n",
        "                                 padding='max_length',\n",
        "                                 truncation=True,\n",
        "                                 max_length=self.max_len,\n",
        "                                 return_tensors='pt')\n",
        "        item = {k: v.squeeze() for k, v in inputs.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "train_dataset = QDataset(train_df['ques_detl'].tolist(), train_df['label'].tolist(), tokenizer)\n",
        "test_dataset = QDataset(test_df['ques_detl'].tolist(), test_df['label'].tolist(), tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from peft) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from peft) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.12/site-packages (from peft) (2.7.0)\n",
            "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (from peft) (4.52.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.12/site-packages (from peft) (1.7.0)\n",
            "Requirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in ./.venv/lib/python3.12/site-packages (from peft) (0.32.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2025.5.0)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.2)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers->peft) (0.21.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.32.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.7.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (0.32.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.5.0)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,453,095 || all params: 110,965,326 || trainable%: 1.3095\n"
          ]
        }
      ],
      "source": [
        "# LoRA/DoRA 都在 PEFT 裏\n",
        "%pip install -U peft           # >= 0.10 才有 use_dora 參數\n",
        "%pip install -U transformers   # 你本來就有\n",
        "%pip install -U accelerate     # 與 Trainer 配合\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "# 5. Model Initialization\n",
        "\n",
        "# 給 BERT 做分類任務常見的幾個線性層；偷懶也可以 target_modules=\"all-linear\"\n",
        "dora_config = LoraConfig(\n",
        "    r=8,                     # rank；小模型 r=8~16 通常夠用\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",             # 只訓練 LoRA / DoRA 權重\n",
        "    task_type=\"SEQ_CLS\",     # 句子分類\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    use_dora=True            # ✨ 關鍵開關\n",
        ")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels= len(label2id)\n",
        ")\n",
        "model = get_peft_model(model, dora_config)\n",
        "model.print_trainable_parameters()  # 檢查只有 DoRA 權重在訓練\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpJoDYM7Lebd",
        "outputId": "d34f857f-3242-4519-cae9-48451c141bca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping pytorch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.7.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (0.32.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.5.0)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.8.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.32.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.32.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'default_data_collator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     36\u001b[39m             collated[key] = torch.stack([item[key] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m collated\n\u001b[32m     39\u001b[39m trainer = Trainer(\n\u001b[32m     40\u001b[39m     model=model,\n\u001b[32m     41\u001b[39m     args=training_args,\n\u001b[32m     42\u001b[39m     train_dataset=train_dataset,\n\u001b[32m     43\u001b[39m     eval_dataset=test_dataset,\n\u001b[32m     44\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     45\u001b[39m     tokenizer=tokenizer,\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     data_collator= \u001b[43mdefault_data_collator\u001b[49m\n\u001b[32m     47\u001b[39m )\n",
            "\u001b[31mNameError\u001b[39m: name 'default_data_collator' is not defined"
          ]
        }
      ],
      "source": [
        "%pip uninstall pytorch  # ← 先移除錯的包\n",
        "%pip install torch --upgrade\n",
        "\n",
        "%pip install -U accelerate\n",
        "%pip install -U transformers\n",
        "%pip install --upgrade transformers\n",
        "\n",
        "\n",
        "# 6. Training Arguments and Trainer Setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./outputs',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy'\n",
        "    \n",
        ")\n",
        "\n",
        "# Define metric computation\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    acc = (preds == labels).mean()\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "def clean_collate_fn(batch):\n",
        "    # 只保留模型需要的欄位\n",
        "    valid_keys = {\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"}\n",
        "    collated = {}\n",
        "    for key in valid_keys:\n",
        "        if key in batch[0]:\n",
        "            collated[key] = torch.stack([item[key] for item in batch])\n",
        "    return collated\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=clean_collate_fn  \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Dascupt/DL_final/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "BertForSequenceClassification.forward() got an unexpected keyword argument 'num_items_in_batch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 7. Train and Evaluate\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     metrics = trainer.evaluate()\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[33m'\u001b[39m\u001b[33meval_accuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/transformers/trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/peft/peft_model.py:1559\u001b[39m, in \u001b[36mPeftModelForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m peft_config.peft_type == PeftType.POLY:\n\u001b[32m   1558\u001b[39m             kwargs[\u001b[33m\"\u001b[39m\u001b[33mtask_ids\u001b[39m\u001b[33m\"\u001b[39m] = task_ids\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1570\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1572\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/DL_final/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mTypeError\u001b[39m: BertForSequenceClassification.forward() got an unexpected keyword argument 'num_items_in_batch'"
          ]
        }
      ],
      "source": [
        "# 7. Train and Evaluate\n",
        "if __name__ == '__main__':\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"Test Accuracy: {metrics['eval_accuracy']:.4f}\")\n",
        "\n",
        "# 8. Save Model and Label Mappings\n",
        "    model.merge_and_unload()\n",
        "    os.makedirs('./saved_model', exist_ok=True)\n",
        "    model.save_pretrained('./saved_model')\n",
        "    tokenizer.save_pretrained('./saved_model')\n",
        "    with open('./saved_model/label2id.json', 'w') as f:\n",
        "        json.dump(label2id, f)\n",
        "    with open('./saved_model/id2label.json', 'w') as f:\n",
        "        json.dump(id2label, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzmf8LAmMDra",
        "outputId": "e34da672-6729-4fa1-e355-d87809a91695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: test_math.png\n",
            "OCR Result:\n",
            " NU E#m \"To, Aga 120 tal -\n",
            "gee $5 RANA BA REF AR\n",
            "\n",
            ", eT o +s an ARH A AG > PLT 66\n",
            "CPE rC a) KALI S 2 SRA?\n",
            "\n",
            "  \n",
            "\n",
            "¢ C\n",
            "Predicted Subject: math\n",
            "Predicted Chapter: 相似形\n",
            "Predicted Subject: social\n",
            "Predicted Chapter: 地理：基本概念與臺灣\n"
          ]
        }
      ],
      "source": [
        "# 9. Inference Pipeline\n",
        "def ocr_to_text(image_path: str) -> str:\n",
        "    # Using pytesseract\n",
        "    img = Image.open(image_path)\n",
        "    text = pytesseract.image_to_string(img, lang='eng')\n",
        "    return text.strip()\n",
        "    # Or using EasyOCR:\n",
        "    # reader = easyocr.Reader(['en'])\n",
        "    # result = reader.readtext(image_path, detail=0)\n",
        "    # return ' '.join(result)\n",
        "\n",
        "\n",
        "def load_classifier(model_dir: str = './saved_model'):\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def predict_text(text: str, tokenizer, model):\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred_id = logits.argmax(dim=-1).item()\n",
        "    label = id2label[pred_id]\n",
        "    subject, chapter = label.split('::')\n",
        "    return subject, chapter\n",
        "\n",
        "\n",
        "def test_image(image_path: str):\n",
        "    print(\"Processing:\", image_path)\n",
        "    text = ocr_to_text(image_path)\n",
        "    print(\"OCR Result:\\n\", text)\n",
        "    tokenizer, model = load_classifier()\n",
        "    subject, chapter_name = predict_text(text, tokenizer, model)\n",
        "    print(f\"Predicted Subject: {subject}\\nPredicted Chapter: {chapter_name}\")\n",
        "\n",
        "def test_text(text):\n",
        "    tokenizer, model = load_classifier()\n",
        "    subject, chapter_name = predict_text(text, tokenizer, model)\n",
        "    print(f\"Predicted Subject: {subject}\\nPredicted Chapter: {chapter_name}\")\n",
        "\n",
        "# Usage examples\n",
        "if __name__ == '__main__':\n",
        "    # 1. Train and save\n",
        "    #train()\n",
        "    #save_model()\n",
        "\n",
        "    # 2. Test with an example image\n",
        "    test_image('test_math.png')\n",
        "\n",
        "    test = \"根據經濟部水利署的統計，截至2019年底，臺灣40座主要水庫中，淤積率超過30%的共有15座，例如霧社水庫淤積率達74.8%、烏山頭水庫達49.2%。顯示臺灣水庫淤積程度嚴重，影響水庫蓄水功能。下列何項策略最能有效改善上述現象\"\n",
        "    test_text(test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK-6V-gTSRE7"
      },
      "source": [
        "## textCNN pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrTQl50XSoiN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizerFast\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WipQyw_uSeB3"
      },
      "outputs": [],
      "source": [
        "# 3. TextCNN Model Definition\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_sizes=[3,4,5], num_filters=100):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embed_dim,\n",
        "                      out_channels=num_filters,\n",
        "                      kernel_size=k)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        x = self.embedding(input_ids)  # [B, L, D]\n",
        "        x = x.permute(0, 2, 1)        # [B, D, L]\n",
        "        convs = [torch.relu(conv(x)) for conv in self.convs]  # list of [B, F, L-k+1]\n",
        "        pools = [torch.max(c, dim=2)[0] for c in convs]     # list of [B, F]\n",
        "        out = torch.cat(pools, dim=1)                       # [B, F*len]\n",
        "        out = self.dropout(out)\n",
        "        return self.fc(out)\n",
        "\n",
        "# 4. Training and Evaluation Functions\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_cnn = TextCNN(vocab_size=len(tokenizer), embed_dim=128,\n",
        "                    num_classes=len(label2id)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for batch in tqdm(loader, desc='Train'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
        "\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='Eval'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)\n",
        "\n",
        "# 5. Run Training and Compare\n",
        "epochs = 5\n",
        "best_acc = 0\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_epoch(model_cnn, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = eval_epoch(model_cnn, test_loader, criterion)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model_cnn.state_dict(), 'best_textcnn.pt')\n",
        "\n",
        "print(f\"Best TextCNN Val Acc: {best_acc:.4f}\")\n",
        "\n",
        "# 6. Save Mappings for CNN\n",
        "with open('tokenizer_vocab.json', 'w') as f:\n",
        "    json.dump(tokenizer.vocab, f)\n",
        "with open('label2id.json', 'w') as f:\n",
        "    json.dump(label2id, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
